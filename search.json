[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Borja Requena",
    "section": "",
    "text": "Hello there!\nI’m Borja, an engineering physicist with a taste for computation. I’m currently doing my PhD at ICFO - The Institute of Photonic Sciences, where I develop machine learning algorithms to tackle problems in quantum and statistical physics.\nIn my projects, I lean towards the computational side. I love coding and I take great pleasure when things work optimally. Although it is even better when they are also understandable! Therefore, I like to provide my projects and publications with structured code libraries to reproduce my results and, hopefully, provide the reader with a better understanding about the matter.\nBesides computation, I also like the geeky side of computers. I’ve spent more time than I would like to admit gaming and learning how to build my own computers. I also enjoy life outdoors, though! I love climbing and skiing :D"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Borja Requena",
    "section": "Education",
    "text": "Education\n2024 expected - PhD in Machine learning for physics\n   ICFO - The Institute of Photonic Sciences\n2018 - MSc in Intelligent Interactive Systems\n   UPF - Pompeu Fabra University\n2017 - BSc in Engineering Physics\n   UPC BarcelonaTech - Polytechnic University of Catalonia"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Borja Requena",
    "section": "Experience",
    "text": "Experience\nI’m on a bit of a wild ride going from industry to academia and combining pure machine learning and science.\n2019 - Present – PhD Student\n   ICFO - The Institute of Photonic Sciences\n2019 – Research stay\n   QMUL - Queen Mary University of London & Tooso Inc\n   Industrial collaboration that lead to this paper and the\n   posterior acquisition of Tooso by Coveo.\n2018 – Research Intern\n   Telefonica R&D\n   I developed the infrastructure to start a research line in\n   brain-computer interface.\n2016-2017 – Research fellow (two INIREC grants)\n   UPC BarcelonaTech & Banco Santander\n   I studied the electromagnetic properties of sensitive plants\n   to develop brain imaging techniques."
  },
  {
    "objectID": "about.html#teaching",
    "href": "about.html#teaching",
    "title": "Borja Requena",
    "section": "Teaching",
    "text": "Teaching\nI really enjoy teaching and I try to do it as much as I can.\n\nSince 2020, I’ve been teaching the Machine Learning subjects in the MSc in Quantum Science and Technology and the MSc in Photonics.\nIn 2021 I taught part of the Statistics and data analysis course in the MSc of Multidisciplinary Research in Experimental Sciences, which inspired this blogpost series. In the previous year I taught the python programming sessions.\nIn August 2021 I taught the programming sessions in the summer school of machine learning for quantum physics and chemistry in Warsaw.\nFrom 2019 to 2021 I taught physics to first year engineering students at UPC BarcelonaTech."
  },
  {
    "objectID": "posts/2021-03 VMC with RBM/VMC-intro-RBM.html",
    "href": "posts/2021-03 VMC with RBM/VMC-intro-RBM.html",
    "title": "Introduction to variational Monte Carlo with neural network quantum states",
    "section": "",
    "text": "The main power of Monte Carlo methods comes from the capability of computing high-dimensional integrals in large spaces. In physics, this allows us to compute expectation values of the form \\[\\langle f\\rangle = \\int dx p(x)f(x) \\ \\ \\ \\text{or} \\ \\ \\ \\langle f \\rangle = \\sum_{x} p(x)f(x)\\] for continuous and discrete sytems, respectively. Where \\(p\\) is the probability distribution over states \\(x\\) and \\(f\\) is a function of the state, such as its corresponding energy.\nPhysics is benevolent and, generally, the systems of interest only span a tiny bit of their phase space, meaning that \\(p(x)\\simeq 0\\) for most states \\(x\\) and, therefore, most of the terms in the previous sum have a meaningless contribution. With Monte Carlo, rather than accounting for all the possible states \\(x\\), we approximate the expectation value by sampling from \\(p(x)\\). Hence, \\[\\langle f \\rangle \\approx \\frac{1}{N}\\sum_{i=1}^Nf(x_i),\\] where \\(x_i\\) are sampled according to \\(p(x)\\). This is called importance sampling and it allows us to obtain reasonably good approximations with a limitted amount of samples.\n\n\nIn quantum phsyics, a quantity of utmost interest is the expected value of the energy of a system under the action of a Hamiltonian \\(H\\) and a wave function \\(\\Psi(x)\\) in a given basis \\(x\\) \\[\\langle H \\rangle = \\frac{\\langle\\Psi^*|H|\\Psi\\rangle}{\\langle \\Psi|\\Psi\\rangle} = \\frac{\\int dx\\Psi^*(x)H\\Psi(x)}{\\int dx\\Psi^*(x)\\Psi(x)}.\\]\nFrom now on, we will omit the dependency on the state and denote \\(\\Psi\\equiv\\Psi(x)\\) unless needed for clarification. By introducing a term \\(\\frac{\\Psi}{\\Psi}\\) into the numerator, we can rewrite the integral in a convenient way for Monte Carlo integration \\[\\langle H \\rangle = \\frac{\\int \\Psi^*\\frac{\\Psi}{\\Psi}H\\Psi}{\\int \\Psi^*\\Psi} = \\frac{\\int |\\Psi|^2 \\frac{H\\Psi}{\\Psi}}{\\int |\\Psi|^2} = \\int \\rho E_L,\\] where \\(\\rho=\\frac{|\\Psi|^2}{\\int|\\Psi|^2}\\) is the probability density and \\(E_L=\\frac{H\\Psi}{\\Psi}\\) is the so-called local energy.\nHence, the expected energy can be computed via Monte Carlo integration as the expectation value of the local energy over the probability distribution \\(\\rho=\\frac{|\\Psi|^2}{\\int|\\Psi|^2}\\), such that \\[\\langle H\\rangle \\approx \\frac{1}{N}\\sum_{k=1}^NE_L(x_k)\\]\n\n\n\n\n\n\nNote\n\n\n\n\\(x\\) can be any convenient basis for the given problem at hand, it does not refer to position. In the example case that we will be solving in this tutorial, we take the basis \\(\\sigma^z\\) for a spin system."
  },
  {
    "objectID": "posts/2021-03 VMC with RBM/VMC-intro-RBM.html#importance-sampling",
    "href": "posts/2021-03 VMC with RBM/VMC-intro-RBM.html#importance-sampling",
    "title": "Introduction to variational Monte Carlo with neural network quantum states",
    "section": "Importance Sampling",
    "text": "Importance Sampling\nOne of the most important aspects for Monte Carlo integration is the way that importance sampling is done. Markov Chain Monte Carlo (MCMC) is an efficient approach to perform sampling in many dimensions when the probability density \\(p(x)\\) is dominated by a small part of the whole state space.\nSamples are drawn iteratively, forming a Markov Chain, starting from any given state. In order to properly compute the expected value, the Markov Chain needs to converge to the stationary distribution \\(p(x)\\) regardless of the initial state.\nLet \\(t(x\\rightarrow x')\\) be the probability to transition from state \\(x\\) to \\(x'\\) such that \\(\\sum_{x'}t(x\\rightarrow x')=1\\), and \\(p_s(x)\\) the probability to be in state \\(x\\) at step \\(s\\). Then, \\(p_{s+1}(x) = \\sum_{x'}p_s(x')t(x'\\rightarrow x)\\). A stationary probability is obtained when \\(p_s(x)\\) is independent of the step and, therefore, \\[p(x) = \\sum_{x'}p(x')t(x'\\rightarrow x).\\]\nIf a Markov chain is irreducible, the stationary distribution is unique and, if it is also aperiodic, it converges to it. A sufficient condition for stationarity is satsifying the detailed balance condition \\(p(x)t(x\\rightarrow x') = p(x')t(x'\\rightarrow x)\\).\nThe Metropolis-Hastings algorithm {% cite HastingsBiometrika1970%} is built to satisfy detailed balance. This is a very simple algorithm in which we split the transition probability \\(t(x\\rightarrow x')\\) into two factors: the probability to propose or choose the next state \\(c(x\\rightarrow x')\\) and the probability to accept the next state \\(a(x\\rightarrow x')\\) such that \\[t(x\\rightarrow x') = c(x\\rightarrow x')a(x\\rightarrow x').\\] Detailed balance is fulfilled by taking \\(a(x\\rightarrow x')=\\min\\left\\{1, \\frac{p(x')}{p(x)}\\frac{c(x'\\rightarrow x)}{c(x\\rightarrow x')}\\right\\}\\).\nGenerally, the probability to propose a state is symmetric \\(c(x\\rightarrow x')=c(x'\\rightarrow x)\\), as it can be, for instance, the case of randomly flipping a spin in a lattice. In these cases, the acceptance probability is simplified \\[a(x\\rightarrow x') = \\min\\left\\{1, \\frac{p(x')}{p(x)}\\right\\}.\\]\nA Markov Chain is generated by iterating over the following two steps: 1. With a state \\(x\\), propose a new state \\(x'\\) with probability \\(c(x\\rightarrow x')\\) 2. Accept \\(x'\\) with probability \\(a(x\\rightarrow x')\\). If rejected, the next state is \\(x\\).\nThe time it takes for the Markov Chain to converge to the stationary distribution is called thermalisation. In other words, thermalisation is the time it takes to the Markov Chain to forget its initial state. With MCMC we need to wait for the thermalisation to finish before we can start drawing samples from the desired probability distribution. These samples, though, will be highly correlated between one another, thus requiring careful error analysis to be properly handled. We will deal with this later on.\n\nMetropolis-Hastings for quantum systems\nAs it was previously introduced, the expectation value of the energy can be obtained by sampling configurations according to the distribution \\(\\rho=\\frac{|\\Psi|^2}{\\int|\\Psi|^2}\\). Hence, we want to create a Markov Chain that converges to the stationary distribution \\(\\rho\\) and, therefore, the acceptance probabilities need to be defined accordingly \\[a(x\\rightarrow x') = \\min\\left\\{\n1, \\frac{\\rho(x')}{\\rho(x)}\\right\\} = \\min\\left\\{1, \\frac{|\\Psi(x')|^2}{|\\Psi(x)|^2}\\right\\}.\\] Notice that the normalisation factor \\(\\int|\\Psi|^2\\) cancels out. Thus, we never have to worry about normalising probabilities, which would, most times, make the computation intractable."
  },
  {
    "objectID": "posts/2021-03 VMC with RBM/VMC-intro-RBM.html#example---monte-carlo-integration",
    "href": "posts/2021-03 VMC with RBM/VMC-intro-RBM.html#example---monte-carlo-integration",
    "title": "Introduction to variational Monte Carlo with neural network quantum states",
    "section": "Example - Monte Carlo Integration",
    "text": "Example - Monte Carlo Integration\nWith this, we have the tools to compute the expectation value of an observable of a quantum many-body system. As an example, we will take the quantum Ising spin model \\[H = J\\sum_{i=1}^{n-1}\\sigma_{i}^z\\sigma_{i+1}^z + B\\sum_{i=1}^n\\sigma_{i}^x,\\] where \\(\\sigma_i^z, \\sigma_i^x\\) are the Pauli matrices acting on the \\(i\\)-th site, with open boundary conditions.\nThe only thing that is missing is a trial wave function \\(\\Psi\\) to perform the sampling. We will take the Restricted Boltzmann Machine (RBM) ansatz, as introduced in {% cite CarleoScience2017%}, of the form \\[\\Psi(x) = e^{b^Tx}\\prod_{i=1}^{n_h}2\\cosh(c_i + W_{i\\cdot}x),\\] where \\(b, c, W\\) are the visible biases, hidden biases and weight matrix, respectively, and \\(n_h\\) denotes the number of hidden units.\nFor now, we can just take this as a functional ansatz without diving much further into RBMs. The only thing that we need to know is that RBMs have two layers: a visible and a hidden layer. The visible layer corresponds to the physical system, while the hidden layer provides a set of auxiliary parameters that mediate the interaction between physical units. Therefore, the size of the visible layer is fixed by our problem and we need to choose the size of the hidden layer \\(n_h\\). The higher \\(n_h\\), the higher the representability of the ansatz, at the cost of more expensive computations. Since RBMs are universal approximators, we can always improve our solution by increasing \\(n_h\\).\n\n\n\n\n\n\nNote\n\n\n\nThe code is by no means optimized. In fact, almost everything here presented is very suboptimal. It is meant to be easily readable and resemble the equations as much as possible, in order to provide an idea of the overall procedure.\n\n\n\nclass RBM:\n    \"Super simple implementation of an RBM with complex parameters.\"\n    def __init__(self, n_visible, n_hidden):\n        self.n_visible = n_visible\n        self.n_hidden = n_hidden\n        self.reset()\n        \n    def reset(self):\n        \"Reinitializes the complex parameters at random.\"\n        b = np.random.randn(self.n_visible) + 1j*np.random.randn(self.n_visible) # visible bias\n        c = np.random.randn(self.n_hidden) + 1j*np.random.random(self.n_hidden)  # hidden bias\n        W = (np.random.randn(self.n_hidden, self.n_visible) + \n             1j*np.random.randn(self.n_hidden, self.n_visible))                  # weights\n        self.params = np.concatenate((b, c, W.ravel())) / 10\n        \n    @property\n    def b(self): return self.params[:self.n_visible]\n    \n    @property\n    def c(self): return self.params[self.n_visible:self.n_visible+self.n_hidden]\n    \n    @property\n    def W(self): return np.reshape(self.params[self.n_visible+self.n_hidden:], \n                                   (self.n_hidden, self.n_visible))\n\n    def p(self, v):\n        \"Probability amplitude of a visible state `v`. We don't need it for Monte Carlo.\"\n        return np.exp(np.conj(self.b) @ v)*np.prod(np.cosh(self.c + self.W @ v))*2**self.n_hidden\n    \n    def p_ratio(self, v1, v2):\n        \"Probability ratio between state `v2` and reference `v1`\"\n        f1 = np.cosh(self.c + self.W @ v1)\n        f2 = np.cosh(self.c + self.W @ v2)\n        log_diff = np.conj(self.b) @ (v2-v1) + sum(np.log(f2/f1)) # log of ratio for numerical stability\n        return np.exp(log_diff)\n    \n    def p_ratios(self, v1, v2):\n        \"Probability ratio between list of states `v2` and reference state `v1`.\"\n        return [self.p_ratio(v1, v) for v in v2] \n\nLet us define the physical system by choosing the number of spins, the coefficients of the Hamiltonian and the parameters of our wave function. For now, provided that we need a starting point for our trial wave function, we simply take a set of random parameters for our RBM ansatz.\n\nnp.random.seed(7)\nn = 10         # Number of spins\nJ, B = -2, -1  # Hamiltonian\nn_visible, n_hidden = n, 2*n    # RBM size: twice as many hidden neurons\npsi = RBM(n_visible, n_hidden)  # Randomly initialize our anstaz\n\nWith this, we define some functions to make our code more readable.\n\ndef local_energy(x, psi):\n    \"Local energy of Ising spin model.\"\n    # Interaction term\n    couplings = (x[:-1]==x[1:])*2-1\n    e_interaction = J*sum(couplings)\n\n    # Transverse field\n    states_with_flip = [flip(x, i) for i in range(len(x))]\n    e_field = B*sum(psi.p_ratios(x, states_with_flip))\n    return e_interaction + e_field \n    \ndef flip(x, i):\n    \"flips i-th bit of x\"\n    xflip = deepcopy(x)\n    xflip[i] = 1-xflip[i]\n    return xflip\n\nAnd now we are ready to do the Monte Carlo integration. When dealing with spins, new states are obtained by flipping spins, so we need to choose the total amount of samples to draw n_samples and the amount of spin flips n_flips performed to propose a new configuration.\n\nn_samples = 50000\nn_flips = 1\nstate = np.random.randint(0, 2, n) # initial random state\nstates, energies = [], []\n\nfor k in tqdm(range(n_samples)):\n    # Sample new state\n    spin_idx = np.random.randint(0, n, n_flips) \n    new_state = flip(state, spin_idx)\n\n    if np.random.random() <= np.abs(psi.p_ratio(state, new_state))**2:\n        state = deepcopy(new_state)   # Accept new state\n    states.append(state)\n    energies.append(np.real(local_energy(state, psi)))\n\n\n\nCode\nplt.figure(figsize=(12, 5))\nplt.plot(energies[:1000]) # Plot some\nplt.grid()\nplt.tick_params(labelsize=15)\nplt.ylabel(\"Local energy\", fontsize=20)\nplt.xlabel(\"Sample\", fontsize=20)\nplt.title(f\"E = {np.mean(energies):.4f} +- {np.std(energies)/np.sqrt(n_samples):.4f}\");\n\n\n\n\n\nWith this random wave function, we do not observe any thermalisation. The result is the expected energy obtained with our wave function ansatz. Later on, we will see how to optimize its parameters to find the ground state energy."
  },
  {
    "objectID": "posts/2021-03 VMC with RBM/VMC-intro-RBM.html#statistical-analysis-and-autocorrelation-time",
    "href": "posts/2021-03 VMC with RBM/VMC-intro-RBM.html#statistical-analysis-and-autocorrelation-time",
    "title": "Introduction to variational Monte Carlo with neural network quantum states",
    "section": "Statistical analysis and autocorrelation time",
    "text": "Statistical analysis and autocorrelation time\nBecause of the nature of Markov chains, measurements are always correlated to a certain degree. Given that new states are obtained by modifying the previous ones, consecutive states can be highly correlated, although the correlation fades as the number of steps between states increases. The distance at which we can consider two states to be uncorrlated in the Markov chain is the autocorrelation time \\(\\tau\\).\nThe statistical error \\(\\epsilon\\) is obtained via \\[\\epsilon = \\sqrt{\\frac{s_f^2}{N}}, \\ s_f^2 = \\frac{1}{N-1}\\sum_{i=1}^N\\left(f(X_i) - \\langle f \\rangle\\right)^2.\\] These quantities, however, are well defined for uncorrelated samples. Hence, knowing the autocorrelation time \\(\\tau\\), we can compute our estimation values by exclusively taking samples every \\(\\tau\\) (or \\(\\geq\\tau\\)) steps.\n\nBinning analysis\nKnowing the autocorrelation time is extremely important. However, finding the autocorrelation function is too costful and difficult to analyse so, in practice, we rely in the binning analysis of the time series to estimate both \\(\\tau\\) and \\(\\epsilon\\). The main idea is that averages over chunks of the time-series, which are longer than the autocorrelation time, are independent of each other, thus providing the right error estimates.\nProvided that we do not have any prior knowledge about the autocorrelation time, we have to use blocks of increasing lengths until the error estimate converges. We cut the time series into \\(N_B\\) blocks of fixed length \\(k\\) for several values of \\(k=2^0,2^1, 2^2, \\dots\\) With this, we can compute the block average of the \\(i\\)-th block \\[\\langle f \\rangle_{B_i}=\\frac{1}{k}\\sum_{t=1}^kf(x_{(i-1)k+t}).\\] All the blocks have a mean \\[\\langle f\\rangle_B = \\frac{1}{N_B}\\sum_{i=1}^{N_B}\\langle f\\rangle_{B_i},\\] which, when the block length \\(k\\) is larger than the autocorrelation time \\(\\tau\\), allows us to compute the squared statistical error \\[\\epsilon^2\\approx\\frac{s_B^2}{N_B}=\\frac{1}{N_B(N_B-1)}\\sum_{i=1}^{N_B}\\left(\\langle f\\rangle_{B_i} - \\langle f\\rangle_B\\right)^2.\\] If the blocks are independent, \\(\\frac{s_B^2}{N_B}\\) remains constant for increasing values of \\(k\\), although for large \\(k\\) (low \\(N_B\\sim100\\)) statistical fluctuations emerge.\nThe integrated autocorrelation time can be infered from the binning analysis results. Being \\[\\tau=\\frac{1}{2}\\frac{\\frac{s_B^2}{N_B}(k\\rightarrow \\infty)}{\\frac{s_B^2}{N_B}(k=1)}.\\] Bear in mind that the autocorrelation time can change between quantities."
  },
  {
    "objectID": "posts/2021-03 VMC with RBM/VMC-intro-RBM.html#example---binning-analysis",
    "href": "posts/2021-03 VMC with RBM/VMC-intro-RBM.html#example---binning-analysis",
    "title": "Introduction to variational Monte Carlo with neural network quantum states",
    "section": "Example - Binning analysis",
    "text": "Example - Binning analysis\nLet us continue with our previous example and perform the binning analysis in order to properly infer the error and the autocorrelation time out of the Markov Chain that we have already generated.\n\ndef bin_averages(x, bs):\n    \"Bins time-series `x` into `bs` chunks and takes means\"\n    nb = len(x)//bs\n    bin_avg = [np.mean(x[b*bs:(b+1)*bs]) for b in range(nb)]\n    return np.array(bin_avg)\n\n\nks = [2**k for k in range(12)]\nerrors, means, bin_avgs = [], [], []\n\nfor k in ks:\n    bin_avg = bin_averages(energies, k)\n    error = np.sqrt(np.var(bin_avg)/len(bin_avg))\n    errors.append(error)\n    means.append(bin_avg.mean())\n    bin_avgs.append(bin_avg)\n\n\n\nCode\nplt.figure(figsize=(12, 5))\nplt.plot(ks, np.array(errors)**2, 'o-')\nplt.grid()\nplt.tick_params(labelsize=15)\nplt.ylabel(r\"$s_B^2/N_B$\", fontsize=20)\nplt.xlabel(\"Bin size\", fontsize=20);\n\n\n\n\n\nWith the binning analysis, we see that the squared error converges for a bin size of \\(\\sim100\\). For very large bin sizes, the low number of bins incurs some statistical fluctuations. Thus, the result and statistical errors are properly computed for bin sizes \\(64\\leq k\\leq1000\\). We can take any of these values as valid, although the smaller the bin sizes, the lower the overall computational cost. Hence, for future calculations, we will try to keep a bin size that is well converged but not too large, e.g. between \\(100\\) and \\(200\\).\n\n\nCode\nprint(f\"{means[-3]:.4f} +- {errors[-3]:.4f} for k={ks[-3]}\")\n\n\n-9.3152 +- 0.0632 for k=512\n\n\nAs shown before, we can also use these results to infer the autocorrelation time.\n\ntau = (errors[-3]/errors[0])**2; tau\n\n5.243713762799869\n\n\nLet us see the bin averages with the mean and statistical error.\n\n\nCode\nk_idx = -3 # Choose the bin size\nbins = np.arange(len(bin_avgs[k_idx]))\nplt.figure(figsize=(12, 5))\nplt.scatter(bins, bin_avgs[k_idx], s=50, marker='s')\nplt.hlines(means[k_idx], bins[0], bins[-1], linestyles='--', alpha=0.7)\nplt.fill_between(bins, means[k_idx]-errors[k_idx], means[k_idx]+errors[k_idx], color='k', alpha=0.1)\nplt.grid()\nplt.tick_params(labelsize=15)\nplt.title(f\"Bin size {ks[k_idx]}\", fontsize=20)\nplt.ylabel(\"Local energy\", fontsize=20)\nplt.xlabel(\"Bin\", fontsize=20);\n\nk_idx = -2 # Choose the bin size\nbins = np.arange(len(bin_avgs[k_idx]))\nplt.figure(figsize=(12, 5))\nplt.scatter(bins, bin_avgs[k_idx], s=50, marker='s')\nplt.hlines(means[k_idx], bins[0], bins[-1], linestyles='--', alpha=0.7)\nplt.fill_between(bins, means[k_idx]-errors[k_idx], means[k_idx]+errors[k_idx], color='k', alpha=0.1)\nplt.grid()\nplt.tick_params(labelsize=15)\nplt.title(f\"Bin size {ks[k_idx]}\", fontsize=20)\nplt.ylabel(\"Local energy\", fontsize=20)\nplt.xlabel(\"Bin\", fontsize=20);"
  },
  {
    "objectID": "posts/2021-03 VMC with RBM/VMC-intro-RBM.html#monte-carlo-optimization",
    "href": "posts/2021-03 VMC with RBM/VMC-intro-RBM.html#monte-carlo-optimization",
    "title": "Introduction to variational Monte Carlo with neural network quantum states",
    "section": "Monte Carlo optimization",
    "text": "Monte Carlo optimization\nNow that we are able to reliably compute expectation values, we can use the same methods to optimize our variational ansatz according to a target function. In the previous examples, we have taken a completely random wave function, which is to be the starting point of our optimization process to find the ground state wave function.\nWe use the stochastic reconfiguration (SR) method {% cite SorellaJCP2007%} to optimize the parameters of the ansatz, which approximates the natural gradient {% cite AmariBook2006%}. Let our parametrized wavefunction be \\(\\Psi_\\theta\\) with parameters \\(\\theta\\). With SR, the parameter update rule is \\(\\theta_{t+1}=\\theta_t + \\alpha S_t^{-1}F_t\\), where \\(\\alpha\\) is the learning rate, \\(S\\) is an estimation of the Fischer information matrix and \\(F\\) is an estimation of the gradient of a given cost function. The term \\(S^{-1}F\\) is the natural gradient estimation of the cost function with respect to the parameters \\(\\theta\\).\nGiven that the ground state is the one with the lowest possible energy of the system, our cost function will be the expected energy of the system under our parametrized wave function. This way, the parameters of our ansatz will be tuned to approximate the ground state of the system as best as possible.\nLet us define the variational derivatives \\(O\\) with respect to the \\(k\\)-th parameter \\(\\theta_k\\) of our variational ansatz \\(\\Psi_\\theta\\) as the log-derivative of the wave function \\[O_k(x) = \\frac{\\partial}{\\partial\\theta_k}\\log\\left(\\Psi_\\theta(x)\\right)=\\frac{1}{\\Psi_\\theta(x)}\\frac{\\partial\\Psi_\\theta(x)}{\\partial\\theta_k}\\] With this, we can define \\(S\\) as the covariance matrix of the variational derivatives \\(O_k\\) and compute \\(F\\) in terms of the previously introduced local energy \\(E_L(x)\\): \\[ S_{kk'}=\\langle O_k^*O_{k'}\\rangle-\\langle O_k^*\\rangle\\langle O_{k'}\\rangle\\] \\[ F_k= \\langle E_LO_k^*\\rangle - \\langle E_L\\rangle\\langle O_k^*\\rangle\\] As an extra step, there can be introduced a regularization term to increase stability throught the optimization by removing the null diagonal terms of \\(S\\) such that \\(S_{kk} = S_{kk}+\\lambda\\)."
  },
  {
    "objectID": "posts/2021-03 VMC with RBM/VMC-intro-RBM.html#example---monte-carlo-optimization",
    "href": "posts/2021-03 VMC with RBM/VMC-intro-RBM.html#example---monte-carlo-optimization",
    "title": "Introduction to variational Monte Carlo with neural network quantum states",
    "section": "Example - Monte Carlo optimization",
    "text": "Example - Monte Carlo optimization\nLet us put everything together to find the ground state energy of the Ising Hamiltonian used in the previous examples. For the case of an RBM ansatz, the variational derivatives of the parameters can be obtained pretty easily, being: \\[O_{b_j}(s) = \\frac{\\partial}{\\partial b_j}\\log\\left(\\Psi(s)\\right) = s_j \\] \\[O_{c_i}(s) = \\frac{\\partial}{\\partial c_i}\\log\\left(\\Psi(s)\\right) = \\tanh\\left[\\theta_i(s)\\right]\\] \\[O_{W_{ij}}(s) = \\frac{\\partial}{\\partial W_{ij}}\\log\\left(\\Psi(s)\\right) = s_j\\tanh\\left[\\theta_i(s)\\right]\\] Where \\(\\theta_i(s)\\) is the argument of the hyperbolic cosine \\(\\theta_i(s) = c_i + \\sum_j W_{ij}s_j\\).\nWe have seen that the autocorrelation time was \\(\\tau\\sim 5\\). In order to economize the simulation, rather than computing bin averages of \\(\\sim100-200\\) samples, we will draw samples every few steps larger than \\(\\tau\\) so that subsequent measurements are already uncorrelated.\nWe define our custom functions to compute the variational derivatives and covariances, as well as a function that will sample a bunch of states at once.\n\ndef variational_derivative(x, psi):\n    \"Computes variational derivatives for SR\"\n    theta = psi.c + psi.W @ x\n    Ob = x\n    Oc = np.tanh(theta)\n    Ow = Oc[:, None] @ x[None, :]\n    return np.concatenate((Ob, Oc, Ow.ravel()))\n\ndef covariance(x1, x2):\n    \"Computes the covariance between `x1` and `x2`.\"\n    samples = x1.shape[1]\n    m1 = np.mean(x1, axis=1)\n    m2 = np.mean(x2, axis=1) if len(x2.shape)>1 else np.mean(x2)\n    return (x1 @ x2.T)/samples - m1[:,None] @ m2[None,:]\n\ndef sample_block(psi, bs, x0=None, n_flips=1):\n    \"Sample `bs` states according to `psi`.\"\n    state = np.random.randint(0, 2, psi.n_visible) if x0 is None else x0\n    states = []\n    for _ in range(bs):\n        spin_idx = np.random.randint(0, psi.n_visible, n_flips)\n        new_state = flip(state, spin_idx)\n        if np.random.random() <= np.abs(psi.p_ratio(state, new_state))**2:\n            state = deepcopy(new_state)   # Accept new state   \n        states.append(state)\n    return states\n\nWith sample_block we will be taking one energy sample every \\(N=10\\gg\\tau\\) steps. However, we could also take the mini-block averages as samples, which can be easily implemented in the code below. Taking the mini-block average is, probably, a better practice, although it is more computationally expensive, provided that it still requires to compute the local energy for each state. Another way to lower the computational cost is by increasing the amount of spins that are flipped to propose new configurations.\nIt all boils down to a the trade-off between computational cost and accuracy. For this case, a sample every few steps works just fine. I encourage the reader to try out different block sizes, e.g. \\(\\text{bs}\\in[1, 15]\\), and test with mini-block averages and different number of spin flips. Keeping everything else constant, small block sizes should incur into slower and unstable convergence to the minimal energy due to the high correlation between samples. On the other side, excessively large block sizes suffer from the lack of statistics.\nWhen comparing different parameters, remember to set a random seed in order to always start with the same initial condition.\n\nlearning_iterations = 275\nlr = 1e-2\nn_blocks = 150\nthermalise = int(0.1*n_blocks)\nNb = n_blocks - thermalise\nbs = 10\nn_flips = 1\nenergies = []\n\nfor it in tqdm(range(learning_iterations)):\n    EL, O = np.zeros(Nb, dtype=complex), np.zeros((len(psi.params), Nb), dtype=complex)\n    states = sample_block(psi, thermalise*bs, n_flips=n_flips)\n    state = states[-1]\n    for k in range(Nb):\n        batch = sample_block(psi, bs, x0=state, n_flips=n_flips)\n        states += batch\n        state = batch[-1]\n        EL[k] = local_energy(state, psi)\n        O[:, k] = variational_derivative(state, psi)\n    \n    energies.append(EL.mean())\n    F = covariance(O.conj(), EL[None,:])   # Gradient\n    S = covariance(O.conj(), O)            # Fisher info\n    Sinv = np.linalg.pinv(S, rcond=1e-5)   # (pseudo)Inversion\n    d_params = lr*Sinv @ F\n    psi.params -= d_params.squeeze()\n\nLet us plot the expected energy over the optimization steps.\n\n\nCode\nplt.figure(figsize=(12, 5))\nplt.plot(np.real(energies))\nplt.grid()\nplt.tick_params(labelsize=15)\nplt.ylabel(r\"$\\langle H\\rangle$\", fontsize=20)\nplt.xlabel(\"Learning iteration\", fontsize=20);\n\n\n\n\n\nThe algorithm has converged to the ground state energy at around 150 iterations. Let us see the local energies sampled during the last optimization step.\n\n\nCode\nplt.figure(figsize=(12, 5))\nplt.plot(np.real(EL))\nplt.grid()\nplt.tick_params(labelsize=15)\nplt.ylabel(r\"$E_L$\", fontsize=20)\nplt.xlabel(\"Sample\", fontsize=20);\n\n\n\n\n\nWe can see how, by the end of the optimization, the wavefunction mainly provides the ground state and, with some small probability, other states are sampled. To report the ground state energy, we take the result of the last 100 optimization steps, analogously to the binning analysis (the imaginary part should average to zero).\n\n\nCode\nbins = 100\nenergy = np.mean(np.real(energies[-bins:]))\nstatistical_error = np.std(energies[-bins:])/np.sqrt(bins)\nprint(f\"The obtained energy is {energy:.4f}+-{statistical_error:.4f}\")\n\n\nThe obtained energy is -19.4827+-0.0086\n\n\nNotice that we have taken the quantum Ising model in the ferromagnetic phase. Therefore, the ground state is found when all spins are aligned (either all 0 or 1).\n\n\nCode\nprint(f\"The ground state is {state}\")\n\n\nThe ground state is [1 1 1 1 1 1 1 1 1 1]\n\n\nWe can compare this with exact diagonalization, provided that we are solving a small system, in order to see the actual real error. Let us build the Hamiltonian matrix and diagonlaize it.\n\ndef tensor_prod(idx, s, size=10):\n    \"Tensor product of `s` acting on indexes `idx`. Fills rest with Id.\"\n    Id = np.array([[1,0],[0,1]])\n    idx, s = np.array(idx), np.array(s)\n    matrices = [Id if k not in idx else s for k in range(size)]\n    prod = matrices[0]\n    for k in range(1, size):\n        prod = np.kron(prod, matrices[k])\n    return prod\n\nsx = np.array([[0,1],[1,0]])\nsz = np.array([[1,0],[0,-1]])\n    \nH = (J*sum([tensor_prod([k, k+1], sz, size=N) for k in range(N-1)]) + \n     B*sum([tensor_prod(k, sx, size=N) for k in range(N)]))\ne_vals, e_vecs = np.linalg.eigh(H)\n\n\n\nCode\nrelative_error = np.abs((energy-e_vals[0])/e_vals[0])\nprint(f\"The exact ground state energy is {e_vals[0]:.4f}\")\nprint(f\"Relative error between variational energy {energy:.4f} and exact solution {e_vals[0]:.4f}: {relative_error*100:.4f}%\")\n\n\nThe exact ground state energy is -19.5310\nRelative error between variational energy -19.4827 and exact solution -19.5310: 0.2473%\n\n\nWe see that the error is of the order of \\(0.2\\%\\), which means that our ansatz can accurately represent the exact ground state of the system at hand.\nHopefully, this was helpful to anyone starting with Monte Carlo methods :)"
  },
  {
    "objectID": "posts/2021-03 VMC with RBM/VMC-intro-RBM.html#references",
    "href": "posts/2021-03 VMC with RBM/VMC-intro-RBM.html#references",
    "title": "Introduction to variational Monte Carlo with neural network quantum states",
    "section": "References",
    "text": "References\n{% bibliography –cited %}"
  },
  {
    "objectID": "posts/2021-10 Statistics/anova.html",
    "href": "posts/2021-10 Statistics/anova.html",
    "title": "Introduction to analysis of variance (ANOVA)",
    "section": "",
    "text": "This post is part of a series in which we provide an introduction to statistics and data analysis introducing analysis of variance (ANOVA) and regression. They contain most of the teaching material developed for the Msc of Multidisciplinary Research in Experimental Sciences (BIST).\nThe main goal is to provide an intuition about the main ideas behind the different techniques together with simple code examples that implement them from scratch."
  },
  {
    "objectID": "posts/2021-10 Statistics/anova.html#introduction",
    "href": "posts/2021-10 Statistics/anova.html#introduction",
    "title": "Introduction to analysis of variance (ANOVA)",
    "section": "Introduction",
    "text": "Introduction\nIn any field of study, data comes with all kinds of noise and variability. Hence, it is often difficult to compare the data extracted from different experiments straight away, unless there are major obvious differences between them. Let us, first, build some intuition with an example and, then, dive into the formalism.\nFor instance, we may want to compare the performance of different drugs to treat a certain disease. In this case, we would collect data from patients treated with each drug and we would obtain several groups of data, each with their corresponding variability. Patients treated with the same drug may undergo very different recovery processes which depend on several different factors, such as age, health condition, environmental conditions, etc. Therefore, in order to tell whether some drugs work better than others, we must be able to distinguish between the variance in the data due to the usage of different drugs and the variance due to all the other factors.\nBelow we illustrate a couple of examples with synthetic data that show two cases in which we compare three populations with fixed means (black lines) and different variability. In the left one, the main contribution to the variability of the data is due to using different drugs, making it easy to compare them and study their effects. On the contrary, the right one shows a case in which the effect of the drugs is not so obvious. The means are still different, but the question is whether these differences remain significant, that is when they are larger than what we would expect by chance alone. Furthermore, as the variance in each population increases, the error in the estimated mean (red dashed line) increases.\n\n\n\n\n\n\nNote\n\n\n\nIt is important to distinguish between the real mean \\(\\mu\\) (black line) and the sample estimate \\(\\bar{y}\\) (red line). The real mean is inaccessible for us and the best we can do is estimate it from our data. Usually, the more datapoints we gather, the better our estimate will become.\n\n\n\ndef synthetic_data(n, means, stds):\n    \"Create a synthetic dataset with `n` samples per mean and standard deviation.\"\n    return [np.ones(n)*m + std*np.random.randn(n) for m, std in zip(means, stds)]\n\n\nnp.random.seed(7) # Set random seed for reproducibility\nn_i = 10\nmeans = [0.5, 0.75, 0.65]\nstds_easy, stds_hard = [0.01]*3, [0.15]*3\ndatasets = synthetic_data(n_i, means, stds_easy), synthetic_data(n, means, stds_hard)\n\n\n\nCode\nx = np.arange(n_i*len(means))\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\nfor ax, ds in zip(axes, datasets):\n    ax.scatter(x, np.concatenate(ds, axis=0))\n    for i, m in enumerate(means):\n        ax.hlines(m, x[n*i], x[n*(i+1)-1], alpha=0.5)\n        ax.hlines(np.mean(ds[i]), x[n_i*i], x[n_i*(i+1)-1], colors='r', linestyles='--', alpha=0.7)\n    ax.grid()\n    ax.tick_params(labelsize=15)\n    ax.set_ylim(0.1, 1.15)\n    ax.set_ylabel(\"Value\", fontsize=20)\n    ax.set_xlabel(\"Sample\", fontsize=20);\n\n\n\n\n\nHaving seen the previous examples, let us formalize the analysis to see how we treat and quantify these situations."
  },
  {
    "objectID": "posts/2021-10 Statistics/anova.html#anova",
    "href": "posts/2021-10 Statistics/anova.html#anova",
    "title": "Introduction to analysis of variance (ANOVA)",
    "section": "ANOVA",
    "text": "ANOVA\nThe main question we want to address with ANOVA is whether there are significant differences between the means of different groups of data. Formally, we confront the null hypothesis that all groups are equal \\(H_0:\\mu_1=\\mu_2=\\dots=\\mu_a\\), where \\(a\\) is the number of groups, with the hypothesis that, at least, two means are different \\(H_a: \\mu_i\\neq\\mu_j\\) for some \\(i,j\\in [1, a]\\).\nHowever, we do not compare the means right away. Instead, we focus on the variability of the data. The idea is to separate the overall variability in two parts: the variation within the groups and the variation between them. Intuitively, we find significant differences whenever there is little variation within the groups and large variation between them, as in the previous left figure.\n\nVariance\nThe variance measures the deviation of the data around its mean. Hence, we first need to estimate the mean \\(\\bar{y}\\) and then compute the deviations of each data point with respect to it \\(y_i-\\bar{y}\\). However, deviations take positive and negative values adding up to zero, by definition, so we need to consider their sum of squares (SS) instead as a measure of variability for the data \\[SS= \\sum_{i=1}^n (y_i-\\bar{y})^2,\\] which ensures that all the terms in the sum are positive and contribute to the measure of the variation.\nHowever, SS is an extensive measure that increases with the number of samples, which would not allow us to compare the variability between groups of different sizes. Therefore, we need to normalize it, obtaining what’s known as the variance. The sample variance \\(s^2\\) is obtained normalizing the SS by its number of degrees of freedom (DF) such that \\[s^2 = \\frac{1}{n-1}\\sum_{i=1}^n (y_i-\\bar{y})^2.\\] In a distribution with \\(n\\) samples, we only have \\((n-1)\\) DF provided that the deviations are subject to the constraint that they must add up to zero. Intuitively, if we had \\(n-1\\) deviations, we could guess the value of the last one and so it is “not free”.\nIn ANOVA, the variance is often refered to as mean square (MS).\n\n\nPartitioning the variability\nAs previously introduced, we split the overall variability of the data into two components: the variability between and within the different groups. In order to asses whether there are significant differences between groups, we need to be able to compare these two variabilities. If the variability between groups is much larger than the one within groups, we can conclude that they are different. Conversely, if the variability within groups is very large, we cannot tell the groups apart. More precisely, we compare their associated MS by computing the SS for each component and their DF, so let us introduce the following quantities:\n- \\(SS_{total} = \\sum_{i, j}^{a, n_i}(y_{ij} - \\bar{y})^2\\) is the total SS of the samples with respect to the grand mean \\(\\bar{y}\\), which is the mean of all samples. It is, therefore, a measure of the total variability of the data.\n- \\(SS_{within} = \\sum_{i, j}^{a, n_i} (y_{ij} - \\bar{y_i})^2\\) is the SS of the samples with respect to their group mean \\(\\bar{y_i}\\). This quantity is also known as the error SS and measures the variability of the data within the groups. - \\(SS_{between} = \\sum_{i}^a n_i(\\bar{y_i} - \\bar{y})^2\\) is the SS of the group mean with respect to the grand mean. This quantity is also known as the treatment SS and measures the variability of the data between the groups.\nIn the previous equations, the indexes \\(i,j\\) respectively denote group \\(i\\in[1,a]\\) and sample within the group \\(j\\in[1, n_i]\\), where \\(n_i\\) denotes the number of samples in group \\(i\\). This way, we can write the total SS in terms of the other two \\[SS_{total} = SS_{within} + SS_{between}\\]\nIn the plots below we illustrate the distances quantified by each of the introduced terms using the previous example data.\n\n\nCode\nds = np.concatenate(datasets[1], axis=0)\nx = np.arange(ds.size)\nfig, axes = plt.subplots(1, 3, figsize=(16, 4))\n\n# Total SS\ngrand_mean = ds.mean()\naxes[0].scatter(x, ds)\naxes[0].hlines(grand_mean, x[0], x[-1], alpha=0.9)\nfor xi, yi in zip(x, ds):\n    axes[0].vlines(xi, yi, grand_mean, linestyles='--', alpha=0.7)\naxes[0].set_title(r\"$SS_{total}$\", fontsize=20)\n    \naxes[1].scatter(x, ds)\nfor i in range(len(datasets[1])):\n    idx0, idx1 = n_i*i, n_i*(i+1) if i != len(datasets[1])-1 else None\n    s = slice(idx0, idx1)\n    xs, ys = x[s], ds[s]\n    group_mean = np.mean(ds[s])\n    axes[1].hlines(group_mean, xs[0], xs[-1], colors='r', alpha=0.7)\n    axes[2].hlines(group_mean, xs[0], xs[-1], colors='r', alpha=0.7)\n    for xi, yi in zip(xs, ys):\n        axes[1].vlines(xi, yi, group_mean, linestyles='--', alpha=0.7)\n        axes[2].vlines(xi, grand_mean, group_mean, linestyles='--', alpha=0.7)\naxes[1].set_title(r\"$SS_{within}$\", fontsize=20)\n        \naxes[2].hlines(grand_mean, x[0], x[-1], alpha=0.9)\naxes[2].set_title(r\"$SS_{between}$\", fontsize=20)\n\nfor ax in axes:\n    ax.set_ylim(0.1, 1.15)\n    ax.tick_params(labelsize=13)\n    ax.set_ylabel(\"Value\", fontsize=16)\n    ax.set_xlabel(\"Sample\", fontsize=16)\n    ax.grid(alpha=0.5)\n\n\n\n\n\nSimilarly, we can define the MS associated to each SS by dividing with the proper degrees of freedom: - \\(DF_{total} = n-1\\) is the total number of degrees of freedom. - \\(DF_{within} = n-a\\) is the number of degrees of freedom associated to the variation within groups. For each group we have \\((n_i - 1)\\) degrees of freedom. - \\(DF_{between} = a-1\\) is the number of degrees of freedom associated to the variation between groups. For \\(a\\) groups, we can infer the last deviation from the previous \\(a-1\\).\nAnalogously to the SS, DF fulfill the relationship \\(DF_{total} = DF_{within} + DF_{between}\\).\nWith these quantities we can define the MS corresponding to the variation within groups \\(MS_{within} = SS_{within}/DF_{within}\\) and between groups \\(MS_{between} = SS_{between}/DF_{between}\\).\n\n\nF-ratio\nWith the partition of variability that we have introduced, we can compute the F-ratio \\[F=\\frac{MS_{between}}{MS_{within}},\\] which quantifies the relationship between the amount of variation in both regimes. \\(F\\gg 1\\) indicates that there may be large differences between groups and \\(F\\leq 1\\) suggests that there are no differences between them.\nNevertheless, since we are dealing with sample estimates, \\(F\\) is a random variable that can take a wide range of values. This phenomenon is described by the F-distribution, which represents the likelihood to obtain the values of \\(F\\) under the null hypothesis given \\(DF_{within}, DF_{between}\\). With this, we only accept that there are significant differences between groups if \\(F\\) is sufficiently large to be achieved by chance with a probability lower than \\(0.05\\). This is known as an F-test.\nBelow, we provide a couple of examples of F-distribution given \\((2, 27)\\) and \\((9, 40)\\) DF denoting (between, within). The first example is equivalent to having 3 groups with 10 samples each and the second is equivalent to 10 groups with 50 samples each. This distribution has a closed form as shown here.\n\ndef f_distribution(x, d_b, d_w):\n    \"F-distribution given `d_b` between DF and `d_w` within DF.\"\n    return (d_b/d_w)**(d_b/2)*x**(d_b/2-1)*(1+x*d_b/d_w)**(-(d_b+d_w)/2)/beta(d_b/2, d_w/2)\n\n\nx = np.linspace(0, 7, 100)\nfs = f_distribution(x, 2, 27), f_distribution(x, 9, 40)\n\n\n\nCode\nfig, axes = plt.subplots(1, 2, figsize=(15, 3))\n\nfor ax, f in zip(axes, fs):\n    ax.plot(x, f, linewidth=2.5)\n    ax.grid()\n    ax.tick_params(labelsize=15)\n    ax.set_ylim(0, 1.05)\n    ax.set_ylabel(\"Probability density\", fontsize=20)\n    ax.set_xlabel(\"F-ratio\", fontsize=20);"
  },
  {
    "objectID": "posts/2021-10 Statistics/anova.html#example-fertilizers",
    "href": "posts/2021-10 Statistics/anova.html#example-fertilizers",
    "title": "Introduction to analysis of variance (ANOVA)",
    "section": "Example: fertilizers",
    "text": "Example: fertilizers\nLet’s illustrate the concepts that we have introduced above with an example. We consider a field study in which three fertilizers are tested and we want to compare their efficiency. Each fertilizer is used to grow ten plots and, then, the crops are harvested.\nWe will use the data in Table 1.1 from {% cite GrafenBook2002%}, where they provide the yield of each plot in tonnes.\n\nyield_1 = np.array([6.27, 5.36, 6.39, 4.85, 5.99, 7.14, 5.08, 4.07, 4.35, 4.95])\nyield_2 = np.array([3.07, 3.29, 4.04, 4.19, 3.41, 3.75, 4.87, 3.94, 6.28, 3.15])\nyield_3 = np.array([4.04, 3.79, 4.56, 4.55, 4.53, 3.53, 3.71, 7.00, 4.61, 4.55])\nyields = np.stack([yield_1, yield_2, yield_3], axis=0)\n\nLet’s start by visualizing the data and see whether we can clearly identify differences between each group.\n\nn, (a, n_i) = yields.size, yields.shape\nprint(f\"There are {a} groups with {n_i} samples each for a total of {n} samples.\")\ngrand_mean, group_means = yields.mean(), yields.mean(1)\n\nThere are 3 groups with 10 samples each for a total of 30 samples.\n\n\n\n\nCode\nx = np.arange(n)\nplt.plot(figsize=(6, 4))\nplt.scatter(x, yields.ravel())\nplt.hlines(grand_mean, 0, n, alpha=0.5)\nfor i, m in enumerate(group_means):\n    plt.hlines(m, x[n_i*i], x[n_i*(i+1)-1], colors='r', linestyles='--', alpha=0.7) \nplt.grid()\nplt.ylim(2, 8)\nplt.tick_params(labelsize=15)\nplt.ylabel(\"Yield (tonnes)\", fontsize=20)\nplt.xlabel(\"Sample\", fontsize=20);\n\n\n\n\n\nThe gray line indicates the grand mean and the red dashed lines show the sample group means.\nWhile it looks like the first fertilizer provides the highest yield, on average, we cannot draw clear conclusions as many plots treated with fertilizer 1 yield less crops than some treated with other fertilizers. Furthermore, with such a small amount of samples we could have had good or bad “luck” in some cases so we need a more elaborate analysis to determine whether the observed differences in means are statistically relevant or not.\nLet’s implement what we’ve learned!\n\nss_within = np.sum((yields - group_means[:, None])**2)\nss_between = np.sum(n_i*(group_means - grand_mean)**2)\ndf_within, df_between = n - a, a - 1\nms_within, ms_between = ss_within/df_within, ss_between/df_between\nf_ratio = ms_between/ms_within\nprint(f\"F-ratio of {f_ratio:.4f}.\")\n\nF-ratio of 5.7024.\n\n\nWe have found a rather large F-ratio, suggesting that there may be significant differences between fertilizers. However, we have to check the p-value of this F-ratio for the given degrees of freedom.\nTo construct the p-value, we have to check what is the probability that the F-ratio is equal or larger to the obtained value under the null hypothesis. For this, we have to integrate the probability distribution from the given value to \\(+\\infty\\), which we will approximate by a fixed value of 100, given that the probability distribution peaks arround 1. Notice that this senario of 3 groups with 10 samples corresponds to the left figure of the F-distribution above.\nHere, we will perform a very basic numerical integration in which we approximate the area under the curve by trapezoids.\n\nfs = np.linspace(f_ratio, 100, 1000)\nprob = f_distribution(fs, df_between, df_within)\np_value = np.trapz(prob, x=fs)\nprint(f\"p-value {p_value:.4f}\")\n\np-value 0.0086\n\n\nThe obtained p-value is significantly lower than the typical \\(0.05\\) threshold, so we can accept the hypothesis that there are significant differences between fertilizers."
  },
  {
    "objectID": "posts/2021-10 Statistics/anova.html#references",
    "href": "posts/2021-10 Statistics/anova.html#references",
    "title": "Introduction to analysis of variance (ANOVA)",
    "section": "References",
    "text": "References\n{% bibliography –cited %}"
  },
  {
    "objectID": "posts/2021-10 Statistics/regression.html",
    "href": "posts/2021-10 Statistics/regression.html",
    "title": "Statistics perspective on regression",
    "section": "",
    "text": "This post is part of a series in which we provide an introduction to statistics and data analysis introducing analysis of variance (ANOVA) and regression. They contain most of the teaching material developed for the Msc of Multidisciplinary Research in Experimental Sciences (BIST).\nThe main goal is to provide an intuition about the main ideas behind the different techniques together with simple code examples that implement them from scratch."
  },
  {
    "objectID": "posts/2021-10 Statistics/regression.html#introduction",
    "href": "posts/2021-10 Statistics/regression.html#introduction",
    "title": "Statistics perspective on regression",
    "section": "Introduction",
    "text": "Introduction\nIn a regression analysis we aim to find trends or relationships in our data. In the most basic scenario, we have an explanatory variable \\(x\\) which we expect to have some influence over a response variable \\(y\\) and the goal is to find the relationship between them. The explanatory variables can either be continuous or categorical, but the response variable is always continuous. Furthermore, the relationship between the two variables can be an arbitrarily complex function \\(y(x)\\), although in most applications it is a smooth function.\nIn this introductory post we will start by considering a single explanatory continuous variable and a linear relationship of the form \\(y=\\alpha+\\beta x\\), which is known as linear regression."
  },
  {
    "objectID": "posts/2021-10 Statistics/regression.html#linear-regression",
    "href": "posts/2021-10 Statistics/regression.html#linear-regression",
    "title": "Statistics perspective on regression",
    "section": "Linear regression",
    "text": "Linear regression\nAs we have briefly introduced above, in a linear regression setting we only have a single explanatory variable \\(x\\) that follows a linear relationship with the response variable \\(y\\) of the form \\(y=\\alpha+\\beta x\\). A linear regression, while being the most simple form of regression, allows us to identify general trends in our data. Furthermore, any smooth function can be approximated by a straight line over a sufficiently short range. Therefore, in an extreme case, we can approximate any function by a series of linear regressions (not recommendable).\nAs in ANOVA, the true values \\(\\alpha\\) and \\(\\beta\\) are inaccessible and the best we can do is to estimate them from our data, obtaining the coefficients \\(a, b\\) such that \\(y\\approx a+bx\\). Below, we show two examples of linear regression using synthetic data with \\(\\alpha=1, \\beta=-1\\) and different degrees of noise. The black lines indicate the true relationship \\(y=\\alpha + \\beta x\\) and the red lines show the best estimation \\(y=a+bx\\).\n\ndef line(x, a, b):\n    \"Evaluates `a + x*b`\"\n    return a + x*b\n\ndef synthetic_data(x, intercept, slope, std):\n    \"Create a noisy synthetic dataset.\"\n    return line(x, intercept, slope) + std*np.random.randn(len(x))\n\n\nnp.random.seed(7) # Set random seed for reproducibility\nn = 20\nx = np.linspace(-1, 1, n)\na, b = 1, -1\nstd_easy, std_hard = 0.1, 1.2\ndatasets = synthetic_data(x, a, b, std_easy), synthetic_data(x, a, b, std_hard)\n\n\n\nCode\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\nexact_relation = line(x, a, b)\nfor ax, ds in zip(axes, datasets):\n    ax.scatter(x, ds)\n    ax.plot(x, exact_relation, color='k', alpha=0.5)\n    b_est, a_est = np.polyfit(x, ds, 1)\n    ax.plot(x, line(x, a_est, b_est), color='r', linestyle='--', alpha=0.7)\n    ax.grid()\n    ax.tick_params(labelsize=15)\n    ax.set_ylim(-2.5, 3.5)\n    ax.text(-0.49, 2.6, f\"a={a_est:.3f}, b={b_est:.3f}\", fontsize=15)\n    ax.set_ylabel(\"y\", fontsize=20)\n    ax.set_xlabel(\"x\", fontsize=20);\n\n\n\n\n\nIn the left plot, we see that the estimates \\(a,b\\) are pretty accurate, given that the data follows a nearly perfect linear relationship. However, in the right plot the linear relationship is not so clear and we only find a generic trend on the data. Consequently, in this last case, the relative errors in the estimation of the coefficients is an order of magnitude larger than in the previous case.\n\nFinding the best fit\nNow that we have built some intuition about he problem, let us dive into the details of how we find the previous red lines. The best fit is the line that minimizes the sum of all errors to our data points. However, the deviations from the line \\(y_i-y(x_i)\\) can take positive and negative values, as in the calculation of the variance in ANOVA. Hence, we quantify the error by the sum of squares (SS) of the deviations \\[SS = \\sum_{i=1}^n (y_i-y(x_i))^2,\\] where \\(y(x_i)\\) indicates the prediction of the fit line at the point \\(x_i\\). The minimization of the SS gives the name to the algorithm: least squares.\nIn practice, this optimization is usually performed by derivating \\(SS\\) with respect to the function parameters and setting the equations to zero, as in this example. In this case, we would solve the system of equations \\[\n\\begin{align}\n\\frac{\\partial SS}{\\partial a} &= 2\\left(na - \\sum_{i=1}^n y_i - bx_i\\night) = 0 \\\\\n\\frac{\\partial SS}{\\partial b} &= -2\\sum_{i=1}^n x_i(y_i-a-bx_i) = 0\n\\end{align}\n\\] considering that every term in the \\(SS\\) can be expressed as \\((y_i-a-bx_i)^2\\).\n\nx = np.linspace(-1, 1, n)\nstd = 0.2\ny = synthetic_data(x, a, b, std)\nb_est, a_est = np.polyfit(x, y, 1)\nfit = line(x, a_est, b_est)\n\n\n\nCode\nplt.figure(figsize=(7, 4))\nplt.scatter(x, y)\nplt.plot(x, fit, color='r')\nplt.grid()\nplt.text(0.05, 1.65, f\"a={a_est:.4f}, b={b_est:.4f}\", fontsize=15)\nplt.tick_params(labelsize=15)\nplt.ylabel(\"y\", fontsize=20)\nplt.xlabel(\"x\", fontsize=20);\n\n\n\n\n\n\n\nRegression analysis\n\nANOVA\nIn order to perform the analysis, we can understand the optimization process in a different way:\nAs in ANOVA, we start by fitting the grand mean of the data \\(\\bar{y}\\). This results into a flat line of the form \\(y=\\bar{y}\\) from which the deviations of all points add up to zero. We quantify the error arround the mean with the previously introduced \\(SS_{total}\\) from ANOVA, which we denote as \\(SSY\\) as we shift towards a more general notation.\nThen, we rotate the line around the mean coordinate \\((\\bar{x}, \\bar{y})\\) until we find the one that minimizes the error. Rotating the line, we bring it closer to the data points reducing the overall deviations. Nevertheless, there will still be some variability left to be explained, unless the data follows a perfect line, which we refer to as the error SS denoted \\(SSE\\). This is the equivalent quantity to the \\(SS_{within}\\) introduced in ANOVA, which quantifies the error between our fit and the datapoints.\nThe difference between \\(SSY\\) and \\(SSE\\) is the variability captured by the regression, denoted \\(SSR\\) (analogous to \\(SS_{between}\\)) which quantifies the distance between the grand mean and the fit. Hence, these quantities follow the relationship \\[SSY = SSR + SSE\\]\nIn the plots below we illustrate the distances quantified by each of the terms in a new synthetic dataset.\n\n\nCode\nfig, axes = plt.subplots(1, 3, figsize=(16, 4))\n\n# Total SS\ngrand_mean = y.mean()\naxes[0].scatter(x, y)\naxes[0].hlines(grand_mean, x[0], x[-1], alpha=0.9)\naxes[1].scatter(x, y)\naxes[1].plot(x, fit, color='r', alpha=0.7)\naxes[2].plot(x, fit, color='r', alpha=0.7)\naxes[2].hlines(grand_mean, x[0], x[-1], alpha=0.9)\nfor xi, yi, fi in zip(x, y, fit):\n    axes[0].vlines(xi, yi, grand_mean, linestyles='--', alpha=0.7)\n    axes[1].vlines(xi, yi, fi, linestyles='--', alpha=0.7)\n    axes[2].vlines(xi, fi, grand_mean, linestyles='--', alpha=0.7)\naxes[0].set_title(r\"$SSY$\", fontsize=20)\naxes[1].set_title(r\"$SSE$\", fontsize=20)\naxes[2].set_title(r\"$SSR$\", fontsize=20)\n\nfor ax in axes:\n    ax.set_ylim(-0.22, 2.4)\n    ax.tick_params(labelsize=13)\n    ax.set_ylabel(\"y\", fontsize=16)\n    ax.set_xlabel(\"x\", fontsize=16)\n    ax.grid(alpha=0.5)\n\n\n\n\n\nIn order to perform the proper statistical analysis, we have to take into account the partition of the degrees of freedom (DF). For a dataset with \\(n\\) points, we have \\((n-1)\\) DF for \\(SSY\\) as we know that the deviations are subject to the constraint of adding up to zero. This way, if we know the value of \\((n-1)\\) deviations, we can infer the value of the last one.\nFor \\(SSR\\) we have two DF \\(a, b\\). However, since we have the constraint that the line must go through the mean point \\((\\bar{x}, \\bar{y})\\), finding one parameter immediately sets the other and, therefore, we only have one DF. Hence, there are \\(n-2\\) remaining DF for \\(SSE\\) corresponding to the original \\(n-1\\) DF minus a regression coefficient. This last one can also be understood as that we lose two degrees of freedom estimating \\(a, b\\) from the \\(n\\) data points.\nWith these quantities, we can perform the ANOVA of our regression.\n\ngrand_mean = y.mean()\nsse = np.sum((y - fit)**2)\nssr = np.sum((fit - grand_mean)**2)\ndfe, dfr = n-2, 1\nmse, msr = sse/dfe, ssr/dfr\nf_ratio = msr/mse\nprint(f\"F-ratio of {f_ratio:.4f}.\")\n\nF-ratio of 151.2594.\n\n\nPromising F-ratio! Seems like a good fit, we just need to check a couple more things :)\n\n\nR-squared\nWith the same quantities used for the ANOVA, we can compute the \\(R^2\\) metric. The \\(R^2\\) quantifies the fraction of variability explained by the resulting fit \\[R^2 = \\frac{SSY-SSE}{SSY} = \\frac{SSR}{SSY}.\\] In the case of a perfect fit, \\(SSE=1\\) and \\(R^2=1\\). Hence, values of \\(R^2\\) close to one are indicative of a good fit, while small values may suggest that there are other important variables besides \\(x\\) influencing \\(y\\).\n\nssy = np.sum((y - grand_mean)**2)\nr_sq = ssr/ssy\nprint(f\"R^2={r_sq:.4f}\")\n\nR^2=0.8937\n\n\nQuite high \\(R^2\\) means that most of the variance in the data is captured by our regression. Very good fit!\n\n\nSignificant relationships\nOnce we estimate our coefficients \\(a,b\\) we have to address the question of whether the relationship that we have found is statistically relevant. The strength of the relationship is determined by the resulting slope and so we want to know what are the odds that the obtained slope could have simply happened by chance alone.\nThis way, we formulate our null hypothesis of no relationship \\(\\beta=0\\), as it is quite natural to assume that there is no relationship until the evidence shows otherwise. Then, we compute the standard error of our slope estimation \\(\\varepsilon_b\\), with which we can check how many standard deviations lie between our estimation from the null hypothesis \\(t=b/\\varepsilon_b\\). Finally, we can perform a t-test and accept or reject the null hypothesis accordingly.\nThe standard error of the slope is obtained as \\(\\varepsilon_b=\\sqrt{\\frac{MSE}{SSX}}\\), where \\(MSE=SSE/DFE\\) and \\(SSX=\\sum_{i=1}^n (x_i - \\bar{x})^2\\).\n\nfrom scipy.stats import t\n\n\nssx = np.sum((x - x.mean())**2)\neb = np.sqrt(mse/ssx)\nt_val = b_est/eb\nt_ratio = t.pdf(t_val, df=dfe)\nprint(f\"t-ratio {t_ratio:.2e} for {t_val:.2f} standard deviations away from null hypothesis\")\n\nt-ratio 2.23e-10 for -12.30 standard deviations away from null hypothesis\n\n\n\n\nConfidence intervals\nOur fitting is subject to statistical fluctuations, which means that it is subject to the amount of data that we have access to. Since we can only work with a limited amount of data points, if we were to repeat the whole process given a different sample from the exact same phenomenon our estimation of the regression coefficients \\(a,b\\) would vary. The more data points we have, the smaller these variations would be and, therefore, the more confident we can be about our results.\nTherefore, it is of extreme importance to not only provide our results, but also a measure of the confidence of our estimation. A common practice is to provide intervals in which we are \\(95\\%\\) sure that our estimaties lie within. In regression we can provide confidence intervals for both the coefficients and the predictions.\n\nCoefficient interval\n\nWe have computed the standard error of the slope to assess whether the relationship that we have found is statistically significant. These errors allow us to compute confidence intervals for our regression coefficients in a very simple way following the relationship \\[CI = \\text{coeff}\\pm t_{\\text{crit}}\\varepsilon_{\\text{coeff}}.\\] For instance, in the case of the slope we would report \\[CI_b = \\text{b}\\pm t_{\\text{crit}}\\varepsilon_b,\\] where \\(t_{\\text{crit}}\\) is the value of the \\(t\\)-distribution corresponding to the desired confidence, according to the DF. The DF for these estimates are always the DFE.\n\nt_crit = 2.101 # two-sided 95% for 18 df\nconf = t_crit*eb\nprint(f\"The vlaue of the slope is {b_est:.4f}+-{conf:.4f}\")\nprint(f\"The true value lies in ({b_est-conf:.4f}, {b_est+conf:.4f}) with 95% confidence.\")\n\nThe vlaue of the slope is -0.9723+-0.1661\nThe true value lies in (-1.1384, -0.8062) with 95% confidence.\n\n\n\nPrediction interval\n\nThe same way we have errors and build coefficient interavls in the regression coefficients, we can also provide a measure of confidence for our predictions. In this case, we have to account for two factors influencing the error.\nOn the one hand, even if we assume that our fit \\(y=a+bx\\) matches the true line \\(y=\\alpha+\\beta x\\), there will still be some uncertainty due to scattering around the true line. We can provide a confidence interval just as before \\[CI = y\\pm t_{\\text{crit}}s,\\] where \\(s\\) is the standard deviation of the error \\(s=\\sqrt{MSE}\\).\nOn the other hand, we also have to account for the fact that our fit does not match the exact true line. The prediction error will be heavily affected by the accuracy with which the slope and the intercept have been estimated. The more samples we have, the better estimates of the coefficients and, the closer to the mean point \\((\\bar{x},\\bar{y})\\), the smaller the influence of the slope errors.\nTaking all these aspects into account, we can assign a prediction uncertainty \\(PI\\) to every point \\(x\\) of the form \\[PI(x)=y(x)\\pm t_{\\text{crit}}s\\sqrt{\\frac{1}{m}+\\frac{1}{n}+\\frac{x-\\bar{x}}{SSX}},\\] where every source of uncertainty adds an additional term: \\(\\frac{1}{m}\\) quantifies the scattering around the true line, with \\(m\\) being the number of \\(y\\) values at \\(x\\) (\\(m=1\\) in our case); \\(\\frac{1}{n}\\) quantifies the uncertainty in the intercept estimation \\(a\\), the more datapoints, the lower the error; \\(\\frac{x-\\bar{x}}{SSX}\\) accounts for the uncertainty in the slope, the errors grow as we get further from the mean point.\n\ns = np.sqrt(mse)\nm = 1\nxx = np.linspace(-3, 3, 100)\npred = line(xx, a_est, b_est)\ninterval = t_crit*s*np.sqrt(1/m + 1/n + (xx - x.mean())/ssx)\n\n\n\nCode\nplt.scatter(x, y)\nplt.plot(xx, pred, color='r', alpha=0.7)\nplt.fill_between(xx, pred + interval, pred - interval, alpha=0.15)\nplt.grid()\nplt.tick_params(labelsize=15)\nplt.xlabel(\"x\", fontsize=20)\nplt.ylabel(\"y\", fontsize=20);\n\n\n\n\n\n\n\nInfluential points and outliers\nThe main goal in regression is to minimize the SS from our fit line to the datapoints. As we have seen, all the fit lines go through the mean point \\((\\bar{x}, \\bar{y})\\) and pivot around it. Hence, the error around this piont is rather small and barely influenced by the regression coeffcients.\nOn the contrary, the predictions on the points far from the center are largely influenced by the estimation of the slope and, therefore, they are subject to larger errors. Hence, these points dominate the SS, becoming the most important ones in the minimization. We call these points influential points as they can significantly affect our estimates \\(a, b\\).\nAdditionally, we have to account for outliers, which are points that do not follow the distribution of our data. We can identify these points by standardizing the residuals diving by their standard deviation. This way, we should find \\(95\\%\\) of our samples within 2 standard deviations from the line. The outliers are the remaining \\(5\\%\\) of our data (1 every 20 samples) and, by definition, lie far from the line yielding large errors.\nOutliers can be extremely harmful for our estimation if they are found in an influential position. In some cases, the data is curated by removing the outliers in order to prevent such situations. However, that is not the best practice and, whenever it is performed, there should be reported results before and after the outlier removal.\nBelow we show three cases in which we can see the effect of influential points.\n\n\nCode\nx_out = [1.5, 1.5, 0.1]\ny_out = a + b*x_out[0], 0.6, 0\nfig, axes = plt.subplots(1, 3, figsize=(16, 4))\nfor ax, x_o, y_o in zip(axes, x_out, y_out):\n    x_ext, y_ext = np.concatenate((x, [x_o])), np.concatenate((y, [y_o]))\n    exact_relation = line(x_ext, a, b)\n    ax.scatter(x, y)\n    ax.scatter(x_o, y_o, color='r')\n    ax.plot(x_ext, exact_relation, color='k', alpha=0.5)\n    b_est, a_est = np.polyfit(x_ext, y_ext, 1)\n    ax.plot(x_ext, line(x_ext, a_est, b_est), color='r', linestyle='--', alpha=0.7)\n    ax.grid()\n    ax.tick_params(labelsize=15)\n    ax.set_ylim(-2.5, 3.5)\n    ax.text(-0.49, 2.6, f\"a={a_est:.3f}, b={b_est:.3f}\", fontsize=15)\n    ax.set_ylabel(\"y\", fontsize=20)\n    ax.set_xlabel(\"x\", fontsize=20)\n    \naxes[0].set_title(\"Beneficial\",fontsize=20)\naxes[1].set_title(\"Harmful outlier\", fontsize=20)\naxes[2].set_title(\"Harmless outlier\", fontsize=20);\n\n\n\n\n\nOn the left, we see an influential point that is over the exact distributiond. Then, in the middle, we see an influential point that is an outlier, significantly harming the resulting fit. Finally, on the right, we see an outlier that is found in a position with little influence, barely affecting our fit."
  },
  {
    "objectID": "posts/2021-10 Statistics/regression.html#example-weather-in-szeged",
    "href": "posts/2021-10 Statistics/regression.html#example-weather-in-szeged",
    "title": "Statistics perspective on regression",
    "section": "Example: weather in Szeged",
    "text": "Example: weather in Szeged\nLet’s now perform a regression analysis on a real example. We’ve taken the weather in Szeged 2006-2016 dataset from Kaggle. We will see whether there is any clear relationship between the apparent temperature and the humidity.\nIn order to make things easy, we’ve taken a small sample of the whole dataset with 50 points.\n\nhumidity = np.array([0.98, 0.85, 0.82, 0.75, 0.85, 0.99, 0.44, 0.96, 0.6 , 0.54, 0.73,\n                     0.51, 0.86, 0.78, 0.83, 0.99, 0.75, 0.93, 0.94, 0.77, 0.45, 0.93,\n                     0.66, 0.85, 0.63, 0.79, 0.8 , 0.75, 0.47, 0.86, 0.73, 0.43, 0.81,\n                     0.8 , 0.51, 0.97, 0.66, 0.34, 0.87, 0.93, 0.85, 0.73, 0.5 , 0.92,\n                     0.93, 0.59, 0.56, 0.92, 0.82, 0.92])\n\napp_temp = np.array([-3.5111, -5.7389, 5.1944, -11.1056, -2.7167, 6.4056, 16.1611, 6.1611,\n                     15.7889, 19.9500, 1.1000, 13.8889, 14.9500, 8.2833, 0.2167, 11.0444,\n                     5.2222, 4.5611, 5.5833, 19.2389, 31.900, 0.6333, 22.0611, 17.6389,\n                     5.0444, -1.3444, -6.700, -2.0222, 21.0333, 1.7778, 10.1389, 22.0222,\n                     5.2167, 3.2444, 12.7778, 17.3944, 1.9833, 24.9500, 9.8778, 7.4444,\n                     -2.7333, -15.1722, 22.7778, -5.5611, 5.0889, 6.9444, 22.0611, -2.0944,\n                     5.8056, -2.9222])\n\n\nn = len(app_temp)\ngrand_mean = app_temp.mean()\nb_est, a_est = np.polyfit(humidity, app_temp, 1)\nfit = line(humidity, a_est, b_est)\n\n\n\nCode\nxx = np.linspace(0.2, 1, 100)\nplt.scatter(humidity, app_temp)\nplt.scatter(humidity, fit)\nplt.plot(xx, line(xx, a_est, b_est), color='r')\nplt.hlines(grand_mean, xx[0], xx[-1], alpha=0.5)\nplt.text(0.22, -10, f\"a={a_est:.3f}, b={b_est:.3f}\", fontsize=15)\nplt.grid()\nplt.tick_params(labelsize=15)\nplt.xlabel(\"Humidity\", fontsize=20)\nplt.ylabel(\"Apparent temperature\", fontsize=20);\n\n\n\n\n\n\nssy = np.sum((app_temp - grand_mean)**2)\nsse = np.sum((app_temp - fit)**2)\nssr = np.sum((fit - grand_mean)**2)\nssx = np.sum((humidity - humidity.mean())**2)\ndfe, dfr = n-2, 1\nmse, msr = sse/dfe, ssr/dfr\nf_ratio = msr/mse\nr_sq = ssr/ssy\neb = np.sqrt(mse/ssx)\nt_val = b_est/eb\nt_ratio = t.pdf(t_val, df=dfe)\nprint(f\"F   = {f_ratio:.4f}\")\nprint(f\"R^2 = {r_sq:.4f}\")\nprint(f\"t-ratio = {t_ratio:.2e} for {t_val:.2f} standard deviations away from null hypothesis\")\n\nF   = 24.6608\nR^2 = 0.3394\nt-ratio = 1.54e-05 for -4.97 standard deviations away from null hypothesis\n\n\nWhile a large F indicates that the fit line explains a big part of the variance, the low \\(R^2\\) indicates that there is still a lot of error left and that, most likely, there are other aspects besides humidity influencing the apparent temperature.\n\nt_crit = 2.009 # two-sided 95% for 50 df, should be 48 df\nconf = t_crit*eb\nprint(f\"The vlaue of the slope is {b_est:.4f}+-{conf:.4f}\")\nprint(f\"The true value lies in ({b_est-conf:.4f}, {b_est+conf:.4f}) with 95% confidence.\")\n\nThe vlaue of the slope is -34.1349+-13.8094\nThe true value lies in (-47.9443, -20.3255) with 95% confidence.\n\n\nThe confidence interval for the slope is rather broad, as the sttistical error is quite high (order of 40%). It is not surprising seeing how the data points are spread over the line. Let’s build the prediction interval.\n\ns = np.sqrt(mse)\nm = 1\npred = line(xx, a_est, b_est)\ninterval = t_crit*s*np.sqrt(1/m + 1/n + (xx - x.mean())/ssx)\n\n\n\nCode\nplt.scatter(humidity, app_temp)\nplt.plot(xx, pred, color='r', alpha=0.7)\nplt.fill_between(xx, pred + interval, pred - interval, alpha=0.15)\nplt.grid()\nplt.tick_params(labelsize=15)\nplt.xlabel(\"x\", fontsize=20)\nplt.ylabel(\"y\", fontsize=20);"
  },
  {
    "objectID": "posts/2020-11 Machine Learning Applications/computer-vision.html",
    "href": "posts/2020-11 Machine Learning Applications/computer-vision.html",
    "title": "Machine Learning applications: Computer Vision",
    "section": "",
    "text": "This post is part of a series in which we showcase typical machine learning applications in various fields: computer vision, natural language processing and tabular data (for now). The motivation behind this series was teaching machine learning at the MSc in Photonics (UPC).\nIn this post series, we showcase some of the main or most common applications of machine learning to illustrate the capabilities and state of the field. It is mainly intended to provide examples for those who are not familiar with machine learning or that are starting with it. Hence, the focus is put on the main concept of the task and the results, without diving too deep into the architecture details and parameters. We mainly use fastai to perform the demonstration and we encourage the reader to look at their wonderful course and book.\nWe leverage pre-trained models that have been trained for other tasks in massive datasets. Fine-tuning these models to work for our various specific tasks, we achieve state-of-the-art results while speeding up the training process by orders of magnitude. This technique is known as transfer learning."
  },
  {
    "objectID": "posts/2020-11 Machine Learning Applications/computer-vision.html#introduction",
    "href": "posts/2020-11 Machine Learning Applications/computer-vision.html#introduction",
    "title": "Machine Learning applications: Computer Vision",
    "section": "Introduction",
    "text": "Introduction\nComputer vision is the field of machine learning that deals with images, be it medical images, our dog’s cutest picture, some grayscale retro portrait or 8k videos. Processing these images, we can perform all sorts of tasks from identifying components within them to creating entirely new ones.\nComputer vision constitutes one of the most successful fields of machine learning. While in other fields, such as natural language processing, the progress has been slower, machines have long shown super-human results in the field of computer vision. For instance, a remarkable work shows how deep learning can use retinal images to detect a patient’s age, gender, smoking status and systolic blood pressure, as well as inferring several risk factors, while we did not know it was even possible, a priori, to infer such information from those images."
  },
  {
    "objectID": "posts/2020-11 Machine Learning Applications/computer-vision.html#image-classification",
    "href": "posts/2020-11 Machine Learning Applications/computer-vision.html#image-classification",
    "title": "Machine Learning applications: Computer Vision",
    "section": "Image classification",
    "text": "Image classification\nThe first task that comes into mind when we talk about computer vision is image classification. It consists on assigning a certain label to a given image among all the possibilities. Here, we illustrate the process with a rather funny example in which we aim to classify pet images into their respective breeds.\nThe dataset contains images of cats and dogs belonging to 37 different breeds. Let’s have a look at some samples to get a better understanding of the task at hand.\n\n\nCode\npath = untar_data(URLs.PETS)\n\npets = DataBlock(blocks = (ImageBlock, CategoryBlock),\n                 get_items = get_image_files, \n                 splitter = RandomSplitter(seed=42),\n                 get_y = using_attr(RegexLabeller(r'(.+)_\\d+.jpg$'), 'name'),\n                 item_tfms = Resize(460),\n                 batch_tfms = aug_transforms(size=224, min_scale=0.75))\ndls = pets.dataloaders(path/\"images\")\ndls.show_batch(nrows=2, ncols=4)\n\n\n\n\n\nWe have images of dogs and cats that appear as the unique animal and main body of the image. Our job will be to tell the breed to which the animal belongs to. As mentioned in the introduction, instead of training a whole model from scratch, we will take a pre-trained model and adapt it to classify these cats and dogs.\nSkippable details about transfer learning: > In this case, we will use a resnet34 architecture trained on the Imagnet dataset. To get an idea, ImageNet has +20k classes to distinguish with, including several dog breeds. Given that the original task has a relationship with our current one, we can leverage this prior experience to classify our 37 pet breeds. Nonetheless, we have to adapt the model in order to output the pet breed prediction, changing the last trained layer by a fully connected layer with 37 outputs.At the beginning of the training, the main body of the model is already trained to extract features from images. However, the last bit that we append contains random weights. Therefore, it is convenient to, first, train the last bit for a few epochs before training the whole model at once. What we will do is to freeze the model up to the last, recently appended layer. This way, there are no updates in the pre-trained part and the new part is adapted to work with it. Then, we unfreeze the model, allowing the whole thing to keep learning. Nevertheless, without getting into much detail, it is reasonable to think that the first layers of the model require less training that the final ones, provided that they focus on more generic aspects of the image, such as edges or color gradients. Therefore, we will set the learning rate of the initial layers to be lower than the last layers.The main idea is:- Freeze and train last bit at a high learning rate- Unfreeze and train everything together with lower discriminative learning ratesFastai has a built in functionality learn.fine_tune that handles the freezing/unfreezing for us, which is what we will be using for the rest of the notebook, after this example.\nLet’s train the model!\n\n\nCode\nlearn = cnn_learner(dls, resnet34, metrics=error_rate).to_fp16()\nlearn.freeze()\nlearn.fit_one_cycle(3, lr_max=1e-3, pct_start=0.99)\nlearn.unfreeze()\nlearn.fit_one_cycle(6, lr_max=slice(5e-6, 5e-4)) # Train first layers 100 times slower than last ones\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      3.621868\n      1.109150\n      0.296346\n      00:11\n    \n    \n      1\n      1.263898\n      0.269706\n      0.082544\n      00:11\n    \n    \n      2\n      0.606038\n      0.235287\n      0.078484\n      00:11\n    \n  \n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.372174\n      0.201209\n      0.068336\n      00:13\n    \n    \n      1\n      0.334736\n      0.245479\n      0.078484\n      00:13\n    \n    \n      2\n      0.273055\n      0.197853\n      0.062923\n      00:13\n    \n    \n      3\n      0.178965\n      0.169431\n      0.054804\n      00:13\n    \n    \n      4\n      0.148987\n      0.167509\n      0.058187\n      00:13\n    \n    \n      5\n      0.112826\n      0.166959\n      0.056834\n      00:14\n    \n  \n\n\n\nWith transfer learning, in less than three minutes of training, we can classify pets into 37 breeds with an error rate of about ~5%. Let’s have a look at some of the images in the validation set with the model predictions.\n\n\nCode\nlearn.show_results()\n\n\n\n\n\n\n\n\nAll the examples are properly classified. In order to better grasp the limitations of the model, we can check the confusion matrix, which will tell us what are the hardest breeds to classify for the model. In the confusion matrix, every row represents the actual label of the image, while the columns indicate the prediction of the model. Thus, a perfect classifier would have a diagonal confusion matrix.\n\n\nCode\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix(figsize=(10, 10), dpi=100)\n\n\n\n\n\n\n\n\nThe main error comes from the model mixing up some American pit bull terriers with Staffordshire bull terriers. There also seems to be some confusion between ragdolls and birmans. A quick google search reveals that we would probably do worse, so we can forgive the model for this mistake :)\n\nMulti-label classification\nWithin image classificaiton we, sometimes, encounter applications in which, rather than assigning a single label to each image, we need to provide a list of labels. This is known as multi-label classification and it is typically applied in situations in which we need to enumerate certain categories that appear in the image.\nI find it pretty intuitive to understand this kind of tasks with the analogy of a kid to whom we ask “what do you see in this image?” and the kid enumerates every single thing in it: a tree, a dog, the sun, a lake, grass, a house, etc. Nonetheless, it will not be able to tell us things that it does know yet or that are deemed irrelevant, such as the brand name of the car or that tiny watch in the wrist of a person in an astonishing landscape. In this case, the machine will be our kid and we will tell it exactly which things to identify in the images.\nTo provide an example, we use the PASCAL dataset. Let’s see how it looks like.\n\n\nCode\npath = untar_data(URLs.PASCAL_2007)\ndf = pd.read_csv(path/'train.csv')\ndf.head()\n\n\n\n\n\n\n  \n    \n      \n      fname\n      labels\n      is_valid\n    \n  \n  \n    \n      0\n      000005.jpg\n      chair\n      True\n    \n    \n      1\n      000007.jpg\n      car\n      True\n    \n    \n      2\n      000009.jpg\n      horse person\n      True\n    \n    \n      3\n      000012.jpg\n      car\n      False\n    \n    \n      4\n      000016.jpg\n      bicycle\n      True\n    \n  \n\n\n\n\nWe have a list of images with assigned labels and an indicator telling whether the image belongs to the validation set or not. See that the third image has a label ‘horse person’. It is not a centaur, it’s just two labels: horse and person. Let’s have a look at the image.\n\n\nCode\nf = df.iloc[2, 0]\nimg = Image.open(path/\"train\"/f)\nimg\n\n\n\n\n\nIndeed, in the image, there appears a horse and three people. In this dataset there are 20 categories:\n\n\nCode\nlabels = set()\nfor L in df.labels.unique(): labels.update(L.split(' '))\nprint(labels)\n\n\n{'chair', 'cow', 'aeroplane', 'cat', 'pottedplant', 'person', 'bus', 'diningtable', 'sheep', 'tvmonitor', 'sofa', 'train', 'bird', 'horse', 'dog', 'bottle', 'boat', 'bicycle', 'motorbike', 'car'}\n\n\nHence, among those, the associated categories to the image are ‘horse’ and ‘person’. Let us have a look at some more examples to get an idea of the kind of images that we encounter.\n\n\nCode\ndef get_x(r): return path/'train'/r['fname']\ndef get_y(r): return r['labels'].split(' ')\n\ndef splitter(df):\n    train = df.index[~df['is_valid']].tolist()\n    valid = df.index[df['is_valid']].tolist()\n    return train, valid\n\ndblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock),\n                   splitter=splitter,\n                   get_x=get_x, \n                   get_y=get_y,\n                   item_tfms = RandomResizedCrop(128, min_scale=0.35))\ndls = dblock.dataloaders(df)\n\ndls.show_batch()\n\n\n\n\n\nUnlike in the previous task, where all images had either dogs or pets as main body, in this case, we encounter a wide range of different images going from close portraits to general landscape views with many different objects in them. Nonetheless, we will do the same as in the previous example: take a pre-trained model and adapt it to this specific task.\nSkippable implementation details: >In this case, we take a resnet50, which is larger than the previous resnet34. The architecture is also pre-trained in the Imagenet dataset and we fine tune it for this multi-label classification. The output layer now contains 20 neurons indicating whether each category appears in the sample.\nLet’s train!\n\n\nCode\nlearn = cnn_learner(dls, resnet50, metrics=[accuracy_multi])\nlearn.fine_tune(3, base_lr=3e-3, freeze_epochs=4)\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy_multi\n      time\n    \n  \n  \n    \n      0\n      0.940069\n      0.709672\n      0.613805\n      00:06\n    \n    \n      1\n      0.820035\n      0.558362\n      0.734721\n      00:06\n    \n    \n      2\n      0.598636\n      0.201305\n      0.949064\n      00:06\n    \n    \n      3\n      0.356319\n      0.124212\n      0.957171\n      00:06\n    \n  \n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy_multi\n      time\n    \n  \n  \n    \n      0\n      0.135804\n      0.114497\n      0.959801\n      00:07\n    \n    \n      1\n      0.117852\n      0.107067\n      0.962769\n      00:07\n    \n    \n      2\n      0.097339\n      0.101569\n      0.964263\n      00:07\n    \n  \n\n\n\nIn less than a minute of training, we are capable of providing all the categories appearing in the given images with an accuracy of ~96%! Let’s have a look at some examples in the validation set.\n\n\nCode\nlearn.show_results()\n\n\n\n\n\n\n\n\nThe model has been quite confused by the forth image in which it had to predict bicycle and pottedplant and, instead, it predicted chair and diningtable. This was a tricky one. Nonetheless, the model has failed in much simpler ones, e.g., in the hrose image, it has predicted two categories: horse and dog, and, in the last one, it has predicted it to be a train, while it had to say bicycle. Despite the confusion, we can visualize the model failures and understand, for instance, how it confused the last image with a train. Hence, we can see that it works consistently with some room for improvement."
  },
  {
    "objectID": "posts/2020-11 Machine Learning Applications/computer-vision.html#image-segmentation",
    "href": "posts/2020-11 Machine Learning Applications/computer-vision.html#image-segmentation",
    "title": "Machine Learning applications: Computer Vision",
    "section": "Image segmentation",
    "text": "Image segmentation\nIn image classification we have related classes with whole images, e.g. telling whether there appears a certain pet breed in the image or whether it contains a horse and a person. In segmentation tasks, instead, the goal is to assign a label to each specific pixel.\nThis technique has numerious applications in various fields. For instance, in autonomous driving we have to tell which parts of the image are road, traffic signs, pedestrians, etc. On a completely different approach, in biomedical imaging, segmentation is used to tell appart healthy tissue from regions affected by certain diseases, such as identifying tumorous cells among healthy ones.\nHere we will show a segmentation example using a subset of the CamVid dataset for autonomous driving. Let us have a look at some examples to get a better understanding of the task at hand.\n\n\nCode\npath = untar_data(URLs.CAMVID_TINY)\ndls = SegmentationDataLoaders.from_label_func(\n    path, bs=8, fnames = get_image_files(path/\"images\"),\n    label_func = lambda o: path/'labels'/f'{o.stem}_P{o.suffix}',\n    codes = np.loadtxt(path/'codes.txt', dtype=str))\n\ndls.show_batch()\n\n\n\n\n\nEach pixel in the images is assigned a label indicating whether it is a tree, a traffic sign, a car, a bike, a building, a pedestrian, etc. Here, we see the images overlapped with the color-coded label mask to ease the visualization. The goal is, given an image, generate the color-coding mask, that is, another image. Again, we will leverage a pre-trained model to perform this task.\nSkippable implementation details: >Just as in the previous cases, we take the same pre-trained resnet34 and adapt it for our task. In this case, we not only limit ourselves to change the last layer of the network but, also, we also modify its architecture. We use the weights from the pre-trained network and convert it into a U-Net.\nLet’s train!\n\n\nCode\nlearn = unet_learner(dls, resnet34)\nlearn.fine_tune(13)\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      3.528453\n      2.278538\n      00:02\n    \n  \n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      1.917065\n      1.561294\n      00:02\n    \n    \n      1\n      1.638206\n      1.185067\n      00:02\n    \n    \n      2\n      1.489484\n      1.176791\n      00:02\n    \n    \n      3\n      1.446579\n      1.413508\n      00:02\n    \n    \n      4\n      1.363572\n      1.037536\n      00:02\n    \n    \n      5\n      1.251376\n      0.994358\n      00:02\n    \n    \n      6\n      1.151080\n      0.793147\n      00:02\n    \n    \n      7\n      1.053900\n      0.740798\n      00:02\n    \n    \n      8\n      0.969674\n      0.706727\n      00:02\n    \n    \n      9\n      0.896650\n      0.705011\n      00:02\n    \n    \n      10\n      0.833096\n      0.699626\n      00:02\n    \n    \n      11\n      0.780772\n      0.693014\n      00:02\n    \n    \n      12\n      0.738692\n      0.689372\n      00:02\n    \n  \n\n\n\nGiven that the dataset is rather small, the training is ridiculously fast. Here, we do not have a metric, such as error rate, that allows us to get an idea of the overall performance. Therefore, we will ask the model to generate the classification mask for some images and see how it goes in a rather qualitative way.\n\n\nCode\nlearn.show_results(max_n=4, figsize=(10, 8))\n\n\n\n\n\n\n\n\nOn the left, we see the real color-coding and, on the right, the model prediction. We can see that all the buildings, trees, cars and traffic signs are consistently colored, so the model is doing a great work here. The most difficult part seems to be the identification of road lines as well as accurately defining the shapes. The model has room for improvement and it would certainly perform better with a larger training dataset.\nBeware, though, the model has entirely missed a cyclist in the first image, WATCH OUT!!"
  },
  {
    "objectID": "posts/2020-11 Machine Learning Applications/computer-vision.html#image-regression",
    "href": "posts/2020-11 Machine Learning Applications/computer-vision.html#image-regression",
    "title": "Machine Learning applications: Computer Vision",
    "section": "Image regression",
    "text": "Image regression\nThe last computer vision application that we will illustrate (for now) is image regression. A regression task is characterized by assigning a real number to a sample. Hence, rather than relating labels to the images, the model will have to provide a continuous value. We encounter this kind of tasks in various different fields, for instance, we can infer the temperature of the soil or other atmospherical properties from satelite images or identify the position of certain objects within them.\nTo provide an example, we will use the Biwi kinect head pose dataset. The dataset is composed of frames from videos of people in which, in each frame, we have the coordinates of their head center. Let us have a look at a few examples to get a better picture of the task.\n\n\nCode\npath = untar_data(URLs.BIWI_HEAD_POSE)\n\ncal = np.genfromtxt(path/'01'/'rgb.cal', skip_footer=6)\ndef img2pose(x): return Path(f'{str(x)[:-7]}pose.txt')\ndef get_ctr(f):\n    ctr = np.genfromtxt(img2pose(f), skip_header=3)\n    c1 = ctr[0] * cal[0][0]/ctr[2] + cal[0][2]\n    c2 = ctr[1] * cal[1][1]/ctr[2] + cal[1][2]\n    return tensor([c1,c2])\n\nbiwi = DataBlock(blocks=(ImageBlock, PointBlock),\n                 get_items=get_image_files,\n                 get_y=get_ctr,\n                 splitter=FuncSplitter(lambda o: o.parent.name=='13'),\n                 batch_tfms=[*aug_transforms(size=(240,320)), \n                             Normalize.from_stats(*imagenet_stats)])\ndls = biwi.dataloaders(path)\ndls.show_batch()\n\n\n\n\n\nEvery image is provided with the 2D relative coordinates of the center of the head with respect to the center of the image, which is represented as a red dot. Hence, every image is assigned two continuous values (2D coordinates) between -1 and +1.\nLet’s have a look at the target prediction of a five samples.\n\n\nCode\nxb, yb = dls.one_batch()\nprint(yb[:5])\n\n\nTensorPoint([[[-0.1538,  0.2342]],\n\n        [[ 0.4655,  0.0642]],\n\n        [[-0.0802, -0.1196]],\n\n        [[ 0.0862, -0.1651]],\n\n        [[-0.1449,  0.0769]]], device='cuda:0')\n\n\nAs in the previous applications, we leverage transfer learning to quickly solve this task.\nSkippable implementation details: >Again, we take a pre-trained resnet34. In this case, provided that the output are two continuous values, there will be two neurons in the output layer. We will restrict the value of each neuron to be within the (-1, 1) range with a sigmoid \\(S(x)\\) activation function such that the output \\(O(x)=S(x)(\\text{high}-\\text{low})+\\text{low}\\). This way, we reduce the amount of things the model needs to learn and prevent it from doing tremenduous mistakes.\nTrain hard!\n\n\nCode\nlearn = cnn_learner(dls, resnet34, y_range=(-1, 1))\nlearn.fine_tune(3, 1e-2)\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.046086\n      0.010338\n      00:42\n    \n  \n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.007814\n      0.002069\n      00:56\n    \n    \n      1\n      0.002781\n      0.000177\n      00:56\n    \n    \n      2\n      0.001354\n      0.000255\n      00:56\n    \n  \n\n\n\nWith four minutes of training we reach a mean squared error (valid_loss) of \\(\\sim10^{-4}\\), meaning that, on average, the error in the predicted position is of the order of \\(\\sim10^{-2}\\). Since we do not know how bad it is to fail in the second decimal, let’s have a look at some examples in order to see whether it is qualitatively awful or it’s something we can’t even tell.\n\n\nCode\nlearn.show_results(nrows=3)\n\n\n\n\n\n\n\n\nThe model does great! Again, on the left, we have the real coordinates and, on the right, we have the prediction. The model error is barely noticeable so, even thogh we could probably refine the model to further lower it, this regressor is perfectly fine to be used within any related application. For instance, this could be used to detect the movement of a player in front of a camera in a virtual reality game."
  },
  {
    "objectID": "posts/2020-11 Machine Learning Applications/computer-vision.html#other-applications",
    "href": "posts/2020-11 Machine Learning Applications/computer-vision.html#other-applications",
    "title": "Machine Learning applications: Computer Vision",
    "section": "Other applications",
    "text": "Other applications\nWithin computer vision, there are other applications that are not covered in this notebook. This is a work in progress and, with time, will become more complete. For instance, we have not covered the new fassionable diffusion models."
  },
  {
    "objectID": "posts/2020-11 Machine Learning Applications/tabular-data.html",
    "href": "posts/2020-11 Machine Learning Applications/tabular-data.html",
    "title": "Machine Learning applications: Tabular Data",
    "section": "",
    "text": "This post is part of a series in which we showcase typical machine learning applications in various fields: computer vision, natural language processing and tabular data (for now). The motivation behind this series was teaching machine learning at the MSc in Photonics (UPC).\nIn this post series, we showcase some of the main or most common applications of machine learning to illustrate the capabilities and state of the field. It is mainly intended to provide examples for those who are not familiar with machine learning or that are starting with it. Hence, the focus is put on the main concept of the task and the results, without diving too deep into the architecture details and parameters. We mainly use fastai to perform the demonstration and we encourage the reader to look at their wonderful course and book."
  },
  {
    "objectID": "posts/2020-11 Machine Learning Applications/tabular-data.html#introduction",
    "href": "posts/2020-11 Machine Learning Applications/tabular-data.html#introduction",
    "title": "Machine Learning applications: Tabular Data",
    "section": "Introduction",
    "text": "Introduction\nTabular data or structured data problems are pretty common in the field of machine learning. It is the prototypical problem in which each sample is described by a certain set of features and, thus, the dataset can be layed out in a table (hence the name). The goal, then, is to predict the value of one of the columns based on the rest. Up until quite recently, tabular data problems where generally addressed with classical models based on decision trees, be it ensembles or gradient boosted machines. However, deep learning has proven quite successful on these tasks in the past years.\nWithin this field, we encounter problems of all kinds, from telling flower types apart given a feature list, to assessing whether to give a loan to a bank client. Unfortunately, tabular data problems are much less nicer to show than computer vision tasks and so this post is slightly more technical than the others in this series. In order to illustrate the process, we will address a regression problem to infer the auction prices of bulldozers that was a kaggle competition. We will solve the same problem with random forests and neural networks in order to see what differences we find with them.\n\n\n\n\n\n\nNote\n\n\n\nWe take a regression example here, but tabular data problems can also be classification tasks and all the processes shown may be applied indistinctively.\n\n\nLet’s have a look at the data.\n\n\nCode\npath = URLs.path('bluebook')\n\ndf = pd.read_csv(path/'TrainAndValid.csv', low_memory=False)\ndf_test = pd.read_csv(path/'Test.csv', low_memory=False)\ndf.head()\n\n\n\n\n\n\n  \n    \n      \n      SalesID\n      SalePrice\n      MachineID\n      ModelID\n      datasource\n      auctioneerID\n      YearMade\n      MachineHoursCurrentMeter\n      UsageBand\n      saledate\n      ...\n      Undercarriage_Pad_Width\n      Stick_Length\n      Thumb\n      Pattern_Changer\n      Grouser_Type\n      Backhoe_Mounting\n      Blade_Type\n      Travel_Controls\n      Differential_Type\n      Steering_Controls\n    \n  \n  \n    \n      0\n      1139246\n      66000.0\n      999089\n      3157\n      121\n      3.0\n      2004\n      68.0\n      Low\n      11/16/2006 0:00\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      Standard\n      Conventional\n    \n    \n      1\n      1139248\n      57000.0\n      117657\n      77\n      121\n      3.0\n      1996\n      4640.0\n      Low\n      3/26/2004 0:00\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      Standard\n      Conventional\n    \n    \n      2\n      1139249\n      10000.0\n      434808\n      7009\n      121\n      3.0\n      2001\n      2838.0\n      High\n      2/26/2004 0:00\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      1139251\n      38500.0\n      1026470\n      332\n      121\n      3.0\n      2001\n      3486.0\n      High\n      5/19/2011 0:00\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      4\n      1139253\n      11000.0\n      1057373\n      17311\n      121\n      3.0\n      2007\n      722.0\n      Medium\n      7/23/2009 0:00\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n5 rows × 53 columns\n\n\n\nEach bulldozer is described by 53 features that constitute the columns of the dataset.\n\n\nCode\ndf.columns\n\n\nIndex(['SalesID', 'SalePrice', 'MachineID', 'ModelID', 'datasource',\n       'auctioneerID', 'YearMade', 'MachineHoursCurrentMeter', 'UsageBand',\n       'saledate', 'fiModelDesc', 'fiBaseModel', 'fiSecondaryDesc',\n       'fiModelSeries', 'fiModelDescriptor', 'ProductSize',\n       'fiProductClassDesc', 'state', 'ProductGroup', 'ProductGroupDesc',\n       'Drive_System', 'Enclosure', 'Forks', 'Pad_Type', 'Ride_Control',\n       'Stick', 'Transmission', 'Turbocharged', 'Blade_Extension',\n       'Blade_Width', 'Enclosure_Type', 'Engine_Horsepower', 'Hydraulics',\n       'Pushblock', 'Ripper', 'Scarifier', 'Tip_Control', 'Tire_Size',\n       'Coupler', 'Coupler_System', 'Grouser_Tracks', 'Hydraulics_Flow',\n       'Track_Type', 'Undercarriage_Pad_Width', 'Stick_Length', 'Thumb',\n       'Pattern_Changer', 'Grouser_Type', 'Backhoe_Mounting', 'Blade_Type',\n       'Travel_Controls', 'Differential_Type', 'Steering_Controls'],\n      dtype='object')\n\n\nThe first thing to do is to identify our target value. In this case, it is the SalePrice column and, in fact, we want to predict the logarithm of the price, as stated in the competition. Then, these problems heavily rely on feature engineering, which consists on adding additional (smart) features that may be informative for the task. For instance, from a single date we can extract the day of the week, whether it was weekend or holidays, beginning or end of the month, etc. We could even figure out the weather if needed!\nCompetitions such as this one are won, in general, by those who can come up with the best relevant features for the task at hand.\n\n\nCode\ndep_var = 'SalePrice'\ndf[dep_var] = np.log(df[dep_var])\n\n\nSkippable impelementation details: >Generally, besides feature engineering, one of the key points in this kind of problems is properly handling categorical and numerical values as well as missing values. For instance, ProductSize is a categorical feature which takes values ‘Large’, ‘Large / Medium’, ‘Medium’, ‘Small’, ‘Mini’ and ‘Compact’. The model does not konw how to process these strings and so we convert them into numerical values assigning a number to each category. These numbers have essentially no meaning. However, given the nature of decision trees, it is convenient that ordinal categories, such as this one, are ordered so that increasing numbers, for example, represent increasing categorical sizes. Numerical values, in turn, should be properly normalized (for neural networks) and, finally, missing values are filled with the mean value of the column and a new column indicating wether it was filled or not is added.\n> > Choosing the right validation set is also extremely important. Given that this is a price forecasting task, we will take the latest sales within the training dataset to be our validation set.\n\n\nCode\nsizes = 'Large','Large / Medium','Medium','Small','Mini','Compact'\ndf['ProductSize'] = df['ProductSize'].astype('category')\ndf['ProductSize'].cat.set_categories(sizes, ordered=True, inplace=True)\n\ndf = add_datepart(df, 'saledate')\ndf_test = add_datepart(df_test, 'saledate')\n\n# Split train/validation\ncond = (df.saleYear<2011) | (df.saleMonth<10)\ntrain_idx, val_idx = np.where(cond)[0], np.where(~cond)[0]\nsplits = (list(train_idx), list(val_idx))\n\n# Handle continuous and categorical variables\nprocs = [Categorify, FillMissing]\ncont, cat = cont_cat_split(df, 1, dep_var=dep_var)\nto = TabularPandas(df, procs, cat, cont, y_names=dep_var, splits=splits)"
  },
  {
    "objectID": "posts/2020-11 Machine Learning Applications/tabular-data.html#random-forests",
    "href": "posts/2020-11 Machine Learning Applications/tabular-data.html#random-forests",
    "title": "Machine Learning applications: Tabular Data",
    "section": "Random forests",
    "text": "Random forests\nRandom forests are the go-to technique to deal with tabular data. They are extremely powerful and extremely easy to set up and train thanks to libraries like sci-kit learn.\nLet’s fit a random forest regressor to the dataset and evaluate its performance. We evaluate the root mean square error (RMSE) of the price prediction on the validation set.\n\n\nCode\ndef rf(xs, y, n_estimators=40, max_samples=200_000, max_features=0.5,\n       min_samples_leaf=5, **kwargs):\n    \"Builds and fits a `RandomForestRegressor`.\"\n    return RandomForestRegressor(n_jobs=-1, n_estimators=n_estimators,\n        max_samples=max_samples, max_features=max_features,\n        min_samples_leaf=min_samples_leaf, oob_score=True).fit(xs, y)\n\ndef r_mse(pred, y):    return round(math.sqrt(((pred-y)**2).mean()), 6)\ndef m_rmse(m, xs, y): return r_mse(m.predict(xs), y)\n\n\nx, y = to.train.xs, to.train.y\nvalid_x, valid_y = to.valid.xs, to.valid.y\n\nm = rf(x, y)\nm_rmse(m, valid_x, valid_y)\n\n\n0.232313\n\n\nThe RMSE is 0.23 in the logarithm of the price. Let’s see how to improve on this. Random forests are quite easy to interpret and we can see, for instance, what are the most relevant features as well as those that are redundant.\nLet’s have a look at the feature importances of the most significant ones (top 30).\n\n\nCode\ndef plot_feature_importances(m, df, top=30):\n    fi = pd.DataFrame({'cols':df.columns, 'imp':m.feature_importances_}).sort_values('imp', ascending=False)\n    fi[:top].plot('cols', 'imp', 'barh', figsize=(12, 8), legend=False)\n    return fi\n\nfi = plot_feature_importances(m, x);\n\n\n\n\n\nWe can see that some features are much more relevant than others. For instance, the year in which the bulldozer was made and its size seem to be the most significant aspects when it comes to determining its selling price, while things such as the transmission mechanism or the day it is being sold barely have an impact.\nWe will remove the least relevant features and retrain our model, leading to a simpler regressor. Therefore, if the performance is similar, it means that it will be able to generalize better. Evaluating the RMSE of the retrained model in the validation set we see that it is not only similar but, actually, a little bit better.\n\n\nCode\nto_keep = fi[fi.imp>0.005].cols\nx_i, valid_x_i = x[to_keep], valid_x[to_keep]\nm = rf(x_i, y)\nm_rmse(m, valid_x_i, valid_y)\n\n\n0.231334\n\n\nBesides feature importance, we can also see which of these features are redundant or provide similar information. Removing redundant features makes our model simpler and more robust, meaning that it will generalize better to unseen data.\n\n\nCode\ndef cluster_columns(df, figsize=(10,6), font_size=12):\n    corr = np.round(scipy.stats.spearmanr(df).correlation, 4)\n    corr_condensed = hc.distance.squareform(1-corr)\n    z = hc.linkage(corr_condensed, method='average')\n    fig = plt.figure(figsize=figsize)\n    hc.dendrogram(z, labels=df.columns, orientation='left', leaf_font_size=font_size)\n    plt.show()\n    \ncluster_columns(x_i)\n\n\n\n\n\nThose features that are merged together at the rightmost part of the plot are the ones that are the most similar. For instance, ‘SaleYear’ and ‘SaleElapsed’ provide the same information but in different formats: the first states the year it was sold and the second tells us how many years have passed since it was sold. Just like with irrelevant features, we can remove some of these redudant ones and re-evaluate our model.\n\n\nCode\nto_drop = ['saleYear', 'ProductGroupDesc', 'fiBaseModel', 'Grouser_Tracks']\nx_ic, valid_x_ic = x_i.drop(to_drop, axis=1), valid_x_i.drop(to_drop, axis=1)\nm = rf(x_ic, y)\nm_rmse(m, valid_x_ic, valid_y)\n\n\n0.232922\n\n\nDropping the least informative features and some of the redundant ones, we have greatly simplified our model while keeping the same performance. This will allow the model to generalize much, much better. We could keep up with the model interpretation and feature engineering, but it is beyond the scope of this post. Some other features that we can drop are time-stamp variables, such as MachineID and SalesID, as well as some model identification ones. This is because, with the model in production, when we want to infer the price of a bulldozer that is currently being sold, the time-stamp-related features do not provide any significant information to the random forest, provided that it is completely unable to generalize beyond what it has seen during training. For an in-depth explanation, check the lesson 7 of fastai’s 2020 course.\nWe will proceed now to do the prediction by training a neural network."
  },
  {
    "objectID": "posts/2020-11 Machine Learning Applications/tabular-data.html#neural-networks",
    "href": "posts/2020-11 Machine Learning Applications/tabular-data.html#neural-networks",
    "title": "Machine Learning applications: Tabular Data",
    "section": "Neural networks",
    "text": "Neural networks\nWhile random forests do great work, they are completely unable to extrapolate to regions beyond the limits of the training data. It may not be the end of the world for some tasks, but it is definitely terrible for some others.\nHowever, as we have seen, those models can be extremely helpful to understand the data and get an idea of the most important features, as they are very easily interpretable. Therefore, we will combine both approaches and take advantage of the feature analysis that we have performed with the random forest. This way, we will get rid of some of the meaningless features straight away before training the network.\nSkippable impelementation details: > The neural network will have to deal with continuous and categorical variables in a completely different way. We will create an embdedding for each categorical variable, while the numerical ones are just input into a fully connected layer. Then, everything is brought together in a dense classifier at the end. Therefore, it is importnat that we split the variables into numerical and categorical and, in fact, categorical variables with high cardinality, like saleElapsed, may be dealt with as numerical ones to prevent massive embeddings.\nLet’s train!\n\n\nCode\nx_ic = x_ic.drop(['SalesID', 'MachineID', 'fiModelDescriptor'], axis=1)\ndf_nn = df[list(x_ic.columns) + [dep_var]] # Keep only useful features\n\ncont_nn, cat_nn = cont_cat_split(df_nn, max_card=9000, dep_var=dep_var)\n\ncont_nn.append('saleElapsed')\ncat_nn.remove('saleElapsed')\ndf_nn.saleElapsed.dtype = int\n\nprocs_nn = [Categorify, FillMissing, Normalize]\nto_nn = TabularPandas(df_nn, procs_nn, cat_nn, cont_nn, splits=splits, y_names=dep_var)\ndls = to_nn.dataloaders(1024)\n\nlearn = tabular_learner(dls, y_range=(8,12), layers=[500,250],\n                        n_out=1, loss_func=F.mse_loss)\n\nlearn.fit_one_cycle(5, 1e-2)\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.070896\n      0.063198\n      00:07\n    \n    \n      1\n      0.056112\n      0.067255\n      00:07\n    \n    \n      2\n      0.049322\n      0.054010\n      00:07\n    \n    \n      3\n      0.043438\n      0.051197\n      00:07\n    \n    \n      4\n      0.040356\n      0.051439\n      00:07\n    \n  \n\n\n\nIn order to compare the random forest with the neural network we have to check what the RMSE is.\n\n\nCode\npreds,targs = learn.get_preds()\nr_mse(preds,targs)\n\n\n\n\n\n0.226801\n\n\nThe neural network provides a much better result than the random forest predicting the sales price of bulldozers. This is, mainly, due to the hard limitation in extrapolation of random forests, which make them struggle in forecasting tasks such as this one where prices evolve through time and we have to make inferences in the future.\nThis has been only one example of how to apply machine learning to tabular data. As you can see, these kind of problems offer a much more engaging relationship in the feature engineering part, provided that we feed the data straight into the classifier."
  },
  {
    "objectID": "posts/2020-11 Machine Learning Applications/nlp.html",
    "href": "posts/2020-11 Machine Learning Applications/nlp.html",
    "title": "Machine Learning applications: Natural Language Processing",
    "section": "",
    "text": "This post is part of a series in which we showcase typical machine learning applications in various fields: computer vision, natural language processing and tabular data (for now). The motivation behind this series was teaching machine learning at the MSc in Photonics (UPC).\nIn this post series, we showcase some of the main or most common applications of machine learning to illustrate the capabilities and state of the field. It is mainly intended to provide examples for those who are not familiar with machine learning or that are starting with it. Hence, the focus is put on the main concept of the task and the results, without diving too deep into the architecture details and parameters. We mainly use fastai to perform the demonstration and we encourage the reader to look at their wonderful course and book.\nWe leverage pre-trained models that have been trained for other tasks in massive datasets. Fine-tuning these models to work for our various specific tasks, we achieve state-of-the-art results while speeding up the training process by orders of magnitude. This technique is known as transfer learning."
  },
  {
    "objectID": "posts/2020-11 Machine Learning Applications/nlp.html#introduction",
    "href": "posts/2020-11 Machine Learning Applications/nlp.html#introduction",
    "title": "Machine Learning applications: Natural Language Processing",
    "section": "Introduction",
    "text": "Introduction\nNatural language processing (NLP) is the field of machine learning that handles the interaction with spoken or written language as us, humans, use it. Hence, in our daily lives, it is, most likely, the field that makes more notable the advances in artificial intelligence, as it is our interface with modern machines. It does not matter that our phone can run the most advanced algorithms if, when we ask for a simple thing, it does not understand us at all or it replies with 70s-robotic-voice. On the other hand, even if the internal processing is not the most advanced, having a smooth interaction makes us feel that whatever artificial intelligence lies within is much more advanced.\nNLP covers many aspects, featuring speech recognition (voice to text), semantic analysis, text generation and text to speech. For now, we will mainly focus on the processing of text and, in the future, include the speech recognition and generation.\nWhile computer vision has been a long established field overcoming human performance, the advances in NLP are much more recent. Contributions in the last few years with algorithms such as ULMFiT, predecessor of the GPT series (GPT, GPT-2 and GPT-3) have brought the field of NLP a step forward."
  },
  {
    "objectID": "posts/2020-11 Machine Learning Applications/nlp.html#language-models",
    "href": "posts/2020-11 Machine Learning Applications/nlp.html#language-models",
    "title": "Machine Learning applications: Natural Language Processing",
    "section": "Language models",
    "text": "Language models\nLanguage models are generative algorithms which are used to create text given a context. Intuitively, these models learn to reproduce the content they read. They have countless wonderful applications, such as this Trump tweet generator, which writes tweets as if it was Donald Trump.\nWe will illustrate the process using the IMDB dataset, which is made of movie reviews. Hence, after training, our model will be able to write movie reviews on its own. The way we do this is to ask the model to infer what the next word will be, given a text fragment. This way, providing the model with a starting point, we add the predicted word to the text and, recursively, repeat the whole process to write a full text.\nJust like in the computer vision examples, we will leverage transfer learning. As starting point, we take generative model that has been trained with the text data from wikipedia, which already comes with great knowledge of our world. Such a pre-trained model is highly versatile as, for instnace, in our application case, movies can feature historical phenomena or specific events and characters that a model with the knowledge from wikipedia will already know. For instance, when the name of an actor appears in a review, the model will already know who it is and other things it has done or, even more, the model will infer the content of the movie out of certain featured names.\nLet’s have a look at the data!\n\n\nCode\npath = untar_data(URLs.IMDB)\nget_imdb = partial(get_text_files, folders=['train', 'test', 'unsup'])\n\ndls_lm = DataBlock(blocks=TextBlock.from_folder(path, is_lm=True),\n                   get_items=get_imdb, splitter=RandomSplitter(0.1)\n                   ).dataloaders(path, path=path, bs=128, seq_len=80)\n\ndls_lm.show_batch(max_n=2)\n\n\n\n\n  \n    \n      \n      text\n      text_\n    \n  \n  \n    \n      0\n      xxbos xxmaj aaah … the xxmaj thing . \\n\\n xxmaj to see a horror film in which not only is every character over the age of thirty , but distinctly unattractive , makes a refreshing change , and reminds me of those distant times when actors were chosen because of their talent and their ability to play realistic characters , rather than because of their teen appeal on a magazine cover . xxmaj and xxmaj carpenter chooses a production designer\n      xxmaj aaah … the xxmaj thing . \\n\\n xxmaj to see a horror film in which not only is every character over the age of thirty , but distinctly unattractive , makes a refreshing change , and reminds me of those distant times when actors were chosen because of their talent and their ability to play realistic characters , rather than because of their teen appeal on a magazine cover . xxmaj and xxmaj carpenter chooses a production designer and\n    \n    \n      1\n      bad - ass fun with one of the greatest shootouts of all time ( nero + xxmaj machine xxmaj gun = xxmaj bad - assery at its best ) . xxmaj wow , what a terrible review . xxbos i personally loved the movie . xxmaj you may not figure it all out , in fact xxmaj i 'm \\n\\n sure that you wo n't figure it all out . xxmaj it 's one of those movies … but it\n      - ass fun with one of the greatest shootouts of all time ( nero + xxmaj machine xxmaj gun = xxmaj bad - assery at its best ) . xxmaj wow , what a terrible review . xxbos i personally loved the movie . xxmaj you may not figure it all out , in fact xxmaj i 'm \\n\\n sure that you wo n't figure it all out . xxmaj it 's one of those movies … but it 's\n    \n  \n\n\n\nNotice that the text is tokenized including special symbols such as xxbos, indicating the beginning of sentence, xxmaj indicating that the next word starts with a capital letter, and so on. Truth is that the model does not understand words as they are, but rather it uses a representation of the words in a high dimensional mathematical space.\nSkippable implementation details: >Looking at the data, we see that the text is split into fragments that are shifted by one word between one another. This is because the target of the text on the left is its next word, that is, the text on the right. We use a recurrent neural network (RNN) to which the text is recursivelly passed word by word and, at every step, the target is the corresponding word on the right. Using the pre-traiend wikipedia RNN, we already have an embedding representing most of the English words and the network knows how to relate them. We just need to fine-tune the RNN in order to specialize it in the movie review context.\nLet’s train!\n\n\nCode\nlearn = language_model_learner(dls_lm, AWD_LSTM, drop_mult=0.3, metrics=[accuracy, Perplexity()]).to_fp16()\nlearn.fine_tune(10, base_lr=2e-2)\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      perplexity\n      time\n    \n  \n  \n    \n      0\n      4.141006\n      4.018666\n      0.290516\n      55.626865\n      11:10\n    \n  \n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      perplexity\n      time\n    \n  \n  \n    \n      0\n      3.806848\n      3.775722\n      0.312664\n      43.628998\n      10:39\n    \n    \n      1\n      3.769517\n      3.749914\n      0.314977\n      42.517441\n      10:32\n    \n    \n      2\n      3.728264\n      3.719217\n      0.319099\n      41.232098\n      10:31\n    \n    \n      3\n      3.677289\n      3.685531\n      0.323200\n      39.866291\n      10:32\n    \n    \n      4\n      3.614626\n      3.654452\n      0.327608\n      38.646320\n      10:32\n    \n    \n      5\n      3.552478\n      3.628734\n      0.330706\n      37.665115\n      10:35\n    \n    \n      6\n      3.487630\n      3.608381\n      0.333443\n      36.906254\n      10:38\n    \n    \n      7\n      3.422164\n      3.594230\n      0.335815\n      36.387676\n      10:36\n    \n    \n      8\n      3.359136\n      3.591810\n      0.336942\n      36.299736\n      10:41\n    \n    \n      9\n      3.309520\n      3.595514\n      0.336873\n      36.434418\n      10:36\n    \n  \n\n\n\nNotice that training these models takes much longer than training the computer vision or tabular data ones. This is because of the nature of the architecture, based on recurrent neural networks (commonly refered to as RNNs), that loop over each word of the text and… well, Python does not like loops :) Unfortunately, the connection to my office computer was interrupted during the training and we can not see the evolution of the training throughout the epochs. However, we can print the last epoch metrics.\n\n\nCode\nlearn.validate()\n\n\n\n\n\n(#3) [3.60433030128479,0.33634933829307556,36.75706100463867]\n\n\nFrom left to right we can see the loss, accuracy and perplexity. The model correctly predicts the next word more than a third of the times. The number may not be outstanding compared with the accuracies that we have obtained in the computer vision tasks, but think about its meaning: the model correctly infers what the next word of any arbitrary review will be one out of every three guesses.\nLet’s now ask it to make some movie reviews for us. We provide it with the start of a sentence and see where it goes. Let’s start with ‘I liked this movie because’:\n\n\nCode\nn_words = 50\nstem = \"I liked this movie because\"\npred = learn.predict(stem, n_words, temperature=0.75)\nprint(pred)\n\n\n\n\n\ni liked this movie because it was one of the best thrillers of the 80 's . The acting was above average and the plot was much more interesting . \n\n There are cuts to this movie that make it seem like it was shot with a camcorder , but i think that\n\n\nSee what happens with ‘Such a terrible movie should never’:\n\n\nCode\nstem = \"Such a terrible movie should never\"\npred = learn.predict(stem, n_words, temperature=0.75)\nprint(pred)\n\n\n\n\n\nSuch a terrible movie should never have been made . The director of this film , Paul Anderson , should be fired . So there are some OTHER reasons to waste any time on this flick . i have to disagree with the other comments . If you want to\n\n\nThese are some hilarious reviews! All of them make sense and are more or less coherent. For instance, when we provide the model with a positive beginning, ‘I liked this movie because’, the movie review tries to explain the reasons why it was so good. On the other hand, when we provide a negative starting point, ‘Such a terrible movie should never’, the review rips the movie beefing its director.\nNotice that, in these two reviews, the model has included names such as Paul Anderson (despite him being an actor and not a director), concepts like camcorder or thriller of the 80’s and it has even emphasized the word ‘OTHER’ with capital letters.\nThe dataset is made out of highly polarized movie reviews ando so the model has an easy time writing positive or negative reviews. Let’s see where it takes us whenever we provide it with an open start like ‘This movie about’:\n\n\nCode\nstem = \"This movie about \"\npred = learn.predict(stem, n_words, temperature=0.75)\nprint(pred)\n\n\n\n\n\nThis movie about a number of things that do n't happen to us , most of them are not exciting . The main problem with the movie is the fact that it is more about life than about the life of others . The main character , Bayliss , is\n\n\n\n\nCode\nstem = \"This movie about \"\npred = learn.predict(stem, n_words, temperature=0.75)\nprint(pred)\n\n\n\n\n\nThis movie about a small town , Iowa , is about a young man ( bill Paxton ) who is right about to go to school with a girl he met and he falls in love with her ( deborah Murray ) . She is also a photographer ,\n\n\nWith the same starting point, the first review is negative straight away, but the second remains neutral for the limited size that we have provided.\nSkippable implementation detail: >We can use this language model for other tasks involving movie reviews, which is one of the foundation of ULMFiT. So we will save the main body of the model for later.\n\n\nCode\nlearn.save_encoder(\"imdb_encoder\")"
  },
  {
    "objectID": "posts/2020-11 Machine Learning Applications/nlp.html#text-classification",
    "href": "posts/2020-11 Machine Learning Applications/nlp.html#text-classification",
    "title": "Machine Learning applications: Natural Language Processing",
    "section": "Text classification",
    "text": "Text classification\nOne of the most extended applications of NLP is text classification, which consists on assigning categories to pieces of text. This is highly related with “understanding” texts in artificial intelligence pipelines and data mining. For instance, we can use these classifiers to automatically sort scientific publications into their respective fields, e.g. condensed matter, neurology, …, and similar tasks. We can also use these models to find whether customer feedback is positive or negative at a large scale, separate fake news from real ones, or even tell the native language of the writer from a text in English.\nIn order to illustrate a case, we will continue with the same application example as before, leveraging the IMDB dataset, to assess whether movie reviews are positive or negative. Notice that, despite using the same dataset, the task is completely different. During the training of the langauge model, the target was the same bit of thext shifted by one word. Now, the target is a label indicating whether the review is positive or negative, as we see below.\n\n\nCode\npath = untar_data(URLs.IMDB)\n\ndls_clas = DataBlock(\n    blocks=(TextBlock.from_folder(path, vocab=dls_lm.vocab), CategoryBlock),\n    get_y = parent_label,\n    get_items=partial(get_text_files, folders=['train', 'test']),\n    splitter=GrandparentSplitter(valid_name='test')\n).dataloaders(path, path=path, bs=128, seq_len=72)\n\ndls_clas.show_batch(max_n=5)\n\n\n\n\n  \n    \n      \n      text\n      category\n    \n  \n  \n    \n      0\n      xxbos xxmaj match 1 : xxmaj tag xxmaj team xxmaj table xxmaj match xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley vs xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley started things off with a xxmaj tag xxmaj team xxmaj table xxmaj match against xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit . xxmaj according to the rules of the match , both opponents have to go through tables in order to get the win . xxmaj benoit and xxmaj guerrero heated up early on by taking turns hammering first xxmaj spike and then xxmaj bubba xxmaj ray . a xxmaj german xxunk by xxmaj benoit to xxmaj bubba took the wind out of the xxmaj dudley brother . xxmaj spike tried to help his brother , but the referee restrained him while xxmaj benoit and xxmaj guerrero\n      pos\n    \n    \n      1\n      xxbos xxmaj titanic directed by xxmaj james xxmaj cameron presents a fictional love story on the historical setting of the xxmaj titanic . xxmaj the plot is simple , xxunk , or not for those who love plots that twist and turn and keep you in suspense . xxmaj the end of the movie can be figured out within minutes of the start of the film , but the love story is an interesting one , however . xxmaj kate xxmaj winslett is wonderful as xxmaj rose , an aristocratic young lady betrothed by xxmaj cal ( billy xxmaj zane ) . xxmaj early on the voyage xxmaj rose meets xxmaj jack ( leonardo dicaprio ) , a lower class artist on his way to xxmaj america after winning his ticket aboard xxmaj titanic in a poker game . xxmaj if he wants something , he goes and gets it\n      pos\n    \n    \n      2\n      xxbos xxrep 3 * xxmaj warning - this review contains \" plot spoilers , \" though nothing could \" spoil \" this movie any more than it already is . xxmaj it really xxup is that bad . xxrep 3 * \\n\\n xxmaj before i begin , xxmaj i 'd like to let everyone know that this definitely is one of those so - incredibly - bad - that - you - fall - over - laughing movies . xxmaj if you 're in a lighthearted mood and need a very hearty laugh , this is the movie for you . xxmaj now without further ado , my review : \\n\\n xxmaj this movie was found in a bargain bin at wal - mart . xxmaj that should be the first clue as to how good of a movie it is . xxmaj secondly , it stars the lame action\n      neg\n    \n    \n      3\n      xxbos xxmaj jim xxmaj carrey is back to much the same role that he played in xxmaj the xxmaj mask , a timid guy who is trying to get ahead in the world but who seems to be plagued with bad luck . xxmaj even when he tries to help a homeless guy from being harassed by a bunch of hoodlums ( and of course they have to be xxmaj mexican , obviously ) , his good will towards his fellow man backfires . xxmaj in that case , it was n't too hard to predict that he was about to have a handful of angry hoodlums , but i like that the movie suggests that things like that should n't be ignored . xxmaj i 'm reminded of the episode of xxmaj michael xxmaj moore 's brilliant xxmaj the xxmaj awful xxmaj truth , when they had a man\n      pos\n    \n    \n      4\n      xxbos xxup the xxup shop xxup around xxup the xxup corner is one of the sweetest and most feel - good romantic comedies ever made . xxmaj there 's just no getting around that , and it 's hard to actually put one 's feeling for this film into words . xxmaj it 's not one of those films that tries too hard , nor does it come up with the oddest possible scenarios to get the two protagonists together in the end . xxmaj in fact , all its charm is innate , contained within the characters and the setting and the plot … which is highly believable to boot . xxmaj it 's easy to think that such a love story , as beautiful as any other ever told , * could * happen to you … a feeling you do n't often get from other romantic comedies\n      pos\n    \n  \n\n\n\nLet’s take the previous language model that we had trained to generate movie reviews, which knows quite a lot of the world thanks to wikipedia, and quite a bit more of movies, as starting point for our classifier.\nSkippable implementation details: > Given the language model, we now add a fully connected layer at the back. The language model acts as feature extractor of the text, which feeds a dense classifier that outputs the probability of belonging to each class: positive or negative. Just as in computer vision, we start by freezing the pre-trained part of the model and then we proceed to unfreeze it once the training has advanced. In this case, however, we will gradually unfreeze the different layers of the model, from back to front, rather than unfreezing it all at once. Additionally, we will be using discriminative learning rates for the whole process.This process of taking a generic language model (wikipedia), fine-tunning it to our task (movie reviews) and, finally, using it as a classifier is the core of ULMFiT, as previously hinted.\nLet’s train!\n\n\nCode\nlearn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5, \n                                metrics=accuracy).to_fp16()\nlearn = learn.load_encoder('imdb_encoder')\n\nlearn.fit_one_cycle(1, 2e-2)\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.356537\n      0.200132\n      0.920920\n      00:25\n    \n  \n\n\n\n\n\nCode\nlearn.freeze_to(-2)\nlearn.fit_one_cycle(1, slice(1e-2/(2.6**4), 1e-2))\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.277847\n      0.178657\n      0.931400\n      00:30\n    \n  \n\n\n\n\n\nCode\nlearn.freeze_to(-3)\nlearn.fit_one_cycle(1, slice(5e-3/(2.6**4), 5e-3))\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.211942\n      0.158949\n      0.941560\n      00:38\n    \n  \n\n\n\n\n\nCode\nlearn.unfreeze()\nlearn.fit_one_cycle(2, slice(1e-3/(2.6**4), 1e-3))\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.181801\n      0.154084\n      0.942280\n      00:46\n    \n    \n      1\n      0.159143\n      0.157385\n      0.944000\n      00:47\n    \n  \n\n\n\nWe reach an accuracy of ~94% in less than 4 minutes of training. This is due to using, first, a generic language model trained on wikipedia, which is then fine-truned for movie reviews and now we can use its prior knowledge to create a wonderful classifier.\nLet’s see some examples.\n\n\nCode\nlearn.show_results(max_n=4)\n\n\n\n\n\n\n\n  \n    \n      \n      text\n      category\n      category_\n    \n  \n  \n    \n      0\n      xxbos xxmaj there 's a sign on xxmaj the xxmaj lost xxmaj highway that says : \\n\\n * major xxup spoilers xxup ahead * \\n\\n ( but you already knew that , did n't you ? ) \\n\\n xxmaj since there 's a great deal of people that apparently did not get the point of this movie , xxmaj i 'd like to contribute my interpretation of why the plot makes perfect sense . xxmaj as others have pointed out , one single viewing of this movie is not sufficient . xxmaj if you have the xxup dvd of xxup md , you can \" cheat \" by looking at xxmaj david xxmaj lynch 's \" top 10 xxmaj hints to xxmaj unlocking xxup md \" ( but only upon second or third viewing , please . ) ;) \\n\\n xxmaj first of all , xxmaj mulholland xxmaj drive is\n      pos\n      pos\n    \n    \n      1\n      xxbos xxmaj back in the mid / late 80s , an xxup oav anime by title of \" bubblegum xxmaj crisis \" ( which i think is a military slang term for when technical equipment goes haywire ) made its debut on video , taking inspiration from \" blade xxmaj runner \" , \" the xxmaj terminator \" and maybe even \" robocop \" , with a little dash of xxmaj batman / xxmaj bruce xxmaj wayne - xxmaj iron xxmaj man / xxmaj tony xxmaj stark and xxmaj charlie 's xxmaj angel 's girl power thrown in for good measure . 8 episodes long , the overall story was that in 21st century xxmaj tokyo , xxmaj japan , year xxunk - xxunk , living machines called xxmaj boomers were doing manual labor and sometimes cause problems . a special , xxup swat like branch of law enforcers ,\n      pos\n      pos\n    \n    \n      2\n      xxbos xxmaj if anyone ever assembles a compendium on modern xxmaj american horror that is truly worth it 's salt , there will * have * to be an entry for xxup sf xxmaj brownrigg 's xxunk xxunk in xxmaj asylum xxmaj horror . xxmaj every time i watch this movie i am impressed by the complete economy of the film , from the compact , totally self - contained plot with a puzzling beginning and an all too horrible ending , the engaging performances by what was essentially a group of non - professional actors , and a xxunk sense of dread and claustrophobia that effectively consumes the xxunk with a certain inevitability which is all the more terrifying because the viewers know what is going on long before the xxunk ] , with the only question being when are they going to wake up & smell the coffee\n      pos\n      pos\n    \n    \n      3\n      xxbos xxmaj tony xxmaj hawk 's xxmaj pro xxmaj skater 2x , is n't much different at all from the previous games ( excluding xxmaj tony xxmaj hawk 3 ) . xxmaj the only thing new that is featured in xxmaj tony xxmaj hawk 's xxmaj pro xxmaj skater 2x , is the new selection of levels , and tweaked out graphics . xxmaj tony xxmaj hawk 's xxmaj pro xxmaj skater 2x offers a new career mode , and that is the 2x career . xxmaj the 2x career is basically xxmaj tony xxmaj hawk 1 career , because there is only about five challenges per level . xxmaj if you missed xxmaj tony xxmaj hawk 1 and 2 , i suggest that you buy xxmaj tony xxmaj hawk 's xxmaj pro xxmaj skater 2x , but if you have played the first two games , you should still\n      pos\n      pos\n    \n  \n\n\n\nWe can even get the classifier to tell whether our reviews are positive or negative.\n\nreview = \"This movie is fucking awful\"\nlearn.predict(review)\n\n\n\n\n('neg', TensorText(0), TensorText([0.9409, 0.0591]))\n\n\n\nreview = \"Such a wonderful movie!!!\"\nlearn.predict(review)\n\n\n\n\n('pos', TensorText(1), TensorText([9.0573e-04, 9.9909e-01]))\n\n\nFrom left to right, we see the category ‘neg’ or ‘pos’, its numerical equivalent (0 or 1) and the tensor telling the machine confidence for each class. In both cases, the model is pretty certain of the class it is predicting: 0.941 negative and 0.999 positive for each review, respectively. Let’s try with something that is less obvious.\n\nreview = (\"At first, this movie looked great and eganging. I was having a great time. Nonetheless,\"+\n          \" the last half turned out to be complete boring and worthless.\")\nlearn.predict(review)\n\n\n\n\n('neg', TensorText(0), TensorText([0.9985, 0.0015]))\n\n\nHere, we introduced a turning point in the review and the model got it pretty well. Let’s try with something a bit more neutral or ambiguous.\n\nreview = \"The movie is about a penguin's life. I guess it is somewhat informative, at least.\"\nlearn.predict(review)\n\n\n\n\n('pos', TensorText(1), TensorText([0.2826, 0.7174]))\n\n\nWe can see that in this last case, the model is much less confident about this being a positive review and the truth is that it can be interpreted in both ways. The movie can fall in the “not bad” category and we say that it is somewhat informative. However, it could also be that the movie is terrible to watch but that, at least, we get to learn something."
  },
  {
    "objectID": "posts/2020-11 Machine Learning Applications/nlp.html#other-applications",
    "href": "posts/2020-11 Machine Learning Applications/nlp.html#other-applications",
    "title": "Machine Learning applications: Natural Language Processing",
    "section": "Other applications",
    "text": "Other applications\nAs previously mentioned, NLP covers a massive range of applications. Directly related with what we’ve done so far, we could do some text regression tasks in which we aim to predict, for instance, a movie score out of the review. Expanding even further, we aim to include speech recognition (speech->text) and generation (text->speech) in the future."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My wonderful blog",
    "section": "",
    "text": "Statistics perspective on regression\n\n\n\n\n\n\n\nStatistics\n\n\nData Analysis\n\n\nGLM\n\n\n\n\nIn a regression analysis, we look for a relationship between two variables. Here, we provide an introduction to linear regression from a statistics point of view with code examples.\n\n\n\n\n\n\nOct 6, 2021\n\n\nBorja Requena\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to analysis of variance (ANOVA)\n\n\n\n\n\n\n\nStatistics\n\n\nData Analysis\n\n\nGLM\n\n\n\n\nANOVA is one of the most fundamental techniques in data analysis. It allows us to tell whether there are statistically relevant differences between several independent groups of data. Here, we provide a conceptual introduction about the technique with code examples from scratch.\n\n\n\n\n\n\nOct 5, 2021\n\n\nBorja Requena\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to variational Monte Carlo with neural network quantum states\n\n\n\n\n\n\n\nMonte Carlo\n\n\nMachine Learning\n\n\n\n\nWe introduce the variational Monte Carlo technique to study quantum many-body systems using neural networks to represent the many-body quantum states. All from scratch!\n\n\n\n\n\n\nMar 24, 2021\n\n\nBorja Requena\n\n\n\n\n\n\n  \n\n\n\n\nMachine Learning applications: Tabular Data\n\n\n\n\n\n\n\nMachine Learning\n\n\nTabular Data\n\n\nfastai\n\n\n\n\nShowcase of typical machine learning applications for tabular data.\n\n\n\n\n\n\nNov 16, 2020\n\n\nBorja Requena\n\n\n\n\n\n\n  \n\n\n\n\nMachine Learning applications: Natural Language Processing\n\n\n\n\n\n\n\nMachine Learning\n\n\nNatural Language\n\n\nfastai\n\n\n\n\nShowcase of typical machine learning applications for natural language processing leveraging transfer learning for fast implementations.\n\n\n\n\n\n\nNov 15, 2020\n\n\nBorja Requena\n\n\n\n\n\n\n  \n\n\n\n\nMachine Learning applications: Computer Vision\n\n\n\n\n\n\n\nMachine Learning\n\n\nComputer Vision\n\n\nfastai\n\n\n\n\nShowcase of simple machine learning applications for computer vision with transfer learning for fast implementations.\n\n\n\n\n\n\nNov 14, 2020\n\n\nBorja Requena\n\n\n\n\n\n\nNo matching items"
  }
]